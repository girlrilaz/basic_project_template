{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################       \n",
    "#Script Name    :                                                                                              \n",
    "#Description    :                                                                                 \n",
    "#Args           :                                                                                           \n",
    "#Author         : Nikhil Rao in R, converted to Python by Nor Raymond                                              \n",
    "#Email          : nraymond@appen.com                                          \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fail Rate Reports for Pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import warnings\n",
    "from functools import reduce\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config_path = \"conf/base\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "except:\n",
    "    \n",
    "    os.chdir('..')\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "# import data_processing module\n",
    "import src.data.data_processing as data_processing\n",
    "# import data_processing module\n",
    "import src.data.data_cleaning as data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_selection(languages):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            language_index = int(input(\"\\nPlease select the number of the Language you are assessing: \"))\n",
    "            if language_index < min(languages.index) or language_index > max(languages.index):\n",
    "                print(f\"\\nYou must enter numbers between {min(languages.index)} - {max(languages.index)}... Please try again\")\n",
    "                continue\n",
    "            elif language_index == \"\":\n",
    "                print(\"\\nYou must enter any numbers\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"\\nYou have selected {language_index} for {languages.iloc[language_index, 0]}\")\n",
    "                language_selected = languages.iloc[language_index, 0]\n",
    "                break\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"\\nYou must enter numerical values only... Please try again\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return language_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Language Modification - getting the overall time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for Language Modification\n",
    "def get_time_taken(df, language_selected):\n",
    "\n",
    "    # Filter data based on selected language\n",
    "    dfr = df[df['Language'] == language_selected]\n",
    "\n",
    "    # Time Taken by Item\n",
    "    dfr[\"Time_Taken_Seconds\"] = (dfr['_created_at'] - dfr['_started_at']).dt.seconds\n",
    "\n",
    "    # Time Taken Overall\n",
    "    dfr_grouped = dfr.groupby('_worker_id').sum('Time_Taken_Seconds')\n",
    "    dfr_grouped[\"Time_Taken_Minutes_Overall\"] = dfr_grouped[\"Time_Taken_Seconds\"] / 60\n",
    "    dfr_grouped = dfr_grouped.reset_index()\n",
    "    dfr = pd.merge(dfr, dfr_grouped[[\"Time_Taken_Minutes_Overall\", \"_worker_id\"]], how = 'left', on = '_worker_id')\n",
    "\n",
    "    return dfr\n",
    "\n",
    "def get_time_taken_all(language_selected, rc, v1, v2):\n",
    "    \n",
    "    df_list = [rc, v1, v2]\n",
    "    keys = [\"rcR\", \"v1R\", \"v2R\"]\n",
    "    df_time = {}\n",
    "    \n",
    "    for df, key in zip(df_list, keys) :\n",
    "\n",
    "        dfr = get_time_taken(df, language_selected)\n",
    "        df_time[key] = dfr\n",
    "\n",
    "    rcR, v1R, v2R = df_time[\"rcR\"], df_time[\"v1R\"], df_time[\"v2R\"]    \n",
    "    \n",
    "    return rcR, v1R, v2R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for calculating Fail Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REPORT 1 : \"Near Exact Match\" - v1_actual_correct_by_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v1_fail_rate(v1R):\n",
    "    \n",
    "    vR_temp = v1R[['Language', 'Fluency', '_worker_id', '_unit_id', 'question_', 'a_domain', 'a_register', \n",
    "                    'wordphrase_a', 'b_domain', 'b_register', 'wordphrase_b', 'difficulty', 'Answer', 'Score']]\n",
    "    \n",
    "    # first grouping\n",
    "    vR_grouped = vR_temp.groupby(['Language', 'Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', \n",
    "                                 'b_domain', 'b_register', 'wordphrase_b', 'difficulty', 'Answer', 'Score'])['_worker_id'].count().reset_index()\n",
    "    vR_grouped = vR_grouped.rename(columns = {\"_worker_id\" : \"Count_of_Test_Takers\"})\n",
    "    \n",
    "    # second grouping\n",
    "    vR_grouped['Total_Test_Takers'] = vR_grouped.groupby(['Language', 'Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', \n",
    "                                    'b_domain', 'b_register', 'wordphrase_b', 'difficulty'])['Count_of_Test_Takers'].transform('sum')   \n",
    "    vR_grouped['Fail_Rate'] = round((vR_grouped['Count_of_Test_Takers'] / vR_grouped['Total_Test_Takers']), 2)\n",
    "    \n",
    "    # filter Score 0 \n",
    "    vR_grouped = vR_grouped[vR_grouped['Score'] == 0]\n",
    "    \n",
    "    # sort values by Market and Fail_rate descending \n",
    "    vR_grouped = vR_grouped.sort_values(['Fluency', 'Fail_Rate'], ascending=[True, False])\n",
    "    \n",
    "    vR_fail_rates = vR_grouped.reset_index(drop=True) #re-order df index\n",
    "    \n",
    "    return vR_fail_rates\n",
    "\n",
    "def generate_report_1(v1R):\n",
    "    \n",
    "    v1_actual_correct_by_question = v1_fail_rate(v1R)\n",
    "    \n",
    "    return v1_actual_correct_by_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REPORT 2 : \"Close Match\" - v2_fail_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_fail_rate(v2R):\n",
    "    \n",
    "    vR_temp = v2R[['Language', 'Fluency', '_worker_id', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', 'b_domain', \n",
    "                   'b_register', 'wordphrase_b', 'difficulty', 'Answers', 'Score']]\n",
    "    \n",
    "    # first grouping\n",
    "    vR_grouped = vR_temp.groupby(['Language', 'Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', 'b_domain', \n",
    "                                  'b_register', 'wordphrase_b', 'difficulty', 'Answers', 'Score'])['_worker_id'].count().reset_index()\n",
    "    vR_grouped = vR_grouped.rename(columns = {\"_worker_id\" : \"Count_of_Test_Takers\"})\n",
    "    \n",
    "    # second grouping\n",
    "    vR_grouped['Total_Test_Takers'] = vR_grouped.groupby(['Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', \n",
    "                                    'b_domain', 'b_register', 'wordphrase_b', 'difficulty'])['Count_of_Test_Takers'].transform('sum')   \n",
    "    vR_grouped['Overall_Fail_Rate'] = round((vR_grouped['Count_of_Test_Takers'] / vR_grouped['Total_Test_Takers']), 2)\n",
    "    \n",
    "    # filter Score 0 \n",
    "    vR_grouped = vR_grouped[vR_grouped['Score'] == 0]\n",
    "    \n",
    "    # sort values by Market and _unit_id \n",
    "    vR_grouped = vR_grouped.sort_values(['Fluency', '_unit_id'], ascending = [True, True])\n",
    "    \n",
    "    # drop Score column\n",
    "    vR_grouped = vR_grouped.drop('Score', axis = 1)\n",
    "    \n",
    "    vR_fail_rates = vR_grouped.reset_index(drop=True) #re-order df index\n",
    "    \n",
    "    return vR_fail_rates\n",
    "\n",
    "def v2_fail_rate_2(v2R):\n",
    "    \n",
    "    vR_temp = v2R[['Language', 'Fluency', '_worker_id', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', 'b_domain', \n",
    "                   'b_register', 'wordphrase_b', 'difficulty', 'rater_answer', 'Answers', 'Score']]\n",
    "    \n",
    "    # first grouping\n",
    "    vR_grouped = vR_temp.groupby(['Language', 'Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', 'b_domain', \n",
    "                                  'b_register', 'wordphrase_b', 'difficulty', 'rater_answer', 'Answers', 'Score'])['_worker_id'].count().reset_index()\n",
    "    vR_grouped = vR_grouped.rename(columns = {\"_worker_id\" : \"Count_of_Test_Takers\"})\n",
    "    \n",
    "    # second grouping\n",
    "    vR_grouped['Total_Test_Takers'] = vR_grouped.groupby(['Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', 'b_domain', \n",
    "                                                          'b_register', 'wordphrase_b', 'difficulty'])['Count_of_Test_Takers'].transform('sum')   \n",
    "    vR_grouped['Rate'] = round((vR_grouped['Count_of_Test_Takers'] / vR_grouped['Total_Test_Takers']), 2)\n",
    "    \n",
    "    # filter Score 0 \n",
    "    vR_grouped = vR_grouped[vR_grouped['Score'] == 0]\n",
    "    \n",
    "    # sort values by Market and _unit_id \n",
    "    vR_grouped = vR_grouped.sort_values(['Fluency', '_unit_id', 'Score', 'Rate'], ascending = [True, True, True, False])\n",
    "    \n",
    "    # drop Score columns\n",
    "    vR_grouped = vR_grouped.drop(['Score', 'Count_of_Test_Takers', 'Total_Test_Takers'], axis = 1)\n",
    "    \n",
    "    vR_fail_rates = vR_grouped.reset_index(drop=True) #re-order df index\n",
    "    \n",
    "    vR_fail_rates  = pd.pivot_table(vR_fail_rates, \n",
    "                           index=['Language', 'Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a', 'b_domain', \n",
    "                                  'b_register', 'wordphrase_b', 'difficulty', 'Answers'],\n",
    "                           values='Rate', columns=['rater_answer']).reset_index()\n",
    "    vR_fail_rates.columns.name = None # remove name for columns\n",
    "    \n",
    "    # remove duplicate rows in the dataframe\n",
    "    vR_fail_rates = vR_fail_rates.drop_duplicates()\n",
    "    \n",
    "    return vR_fail_rates \n",
    "\n",
    "def merge_v2_fail_rates(v2_actual_correct_by_question, v2_actual_correct_by_question_with_answer):\n",
    "    \n",
    "    v2_fail_rates = pd.merge(v2_actual_correct_by_question_with_answer, v2_actual_correct_by_question, how = 'left', \n",
    "                            on = [\"Language\", \"Fluency\", \"_unit_id\", \"question_\", \"a_domain\", \"a_register\", \"wordphrase_a\", \"b_domain\",\n",
    "                                  \"b_register\", \"wordphrase_b\", \"difficulty\", \"Answers\"])\n",
    "    \n",
    "#     v2_fail_rates = v2_fail_rates[['Language', 'Fluency', '_unit_id', 'question_', 'a_domain', 'a_register', 'wordphrase_a',\n",
    "#                 'b_domain', 'b_register', 'wordphrase_b', 'difficulty', 'Count_of_Test_Takers', 'Total_Test_Takers',\n",
    "#                 'Overall_Fail_Rate', 'Answers', 'a_and_b_are_not_related', 'a_and_b_are_related', 'a_and_b_have_the_same_meaning',\n",
    "#                 'a_is_more_specific_than_b', 'b_is_more_specific_than_a']]\n",
    "    \n",
    "    return v2_fail_rates\n",
    "\n",
    "def generate_report_2(v2R):\n",
    "    \n",
    "    v2_actual_correct_by_question = v2_fail_rate(v2R)\n",
    "\n",
    "    v2_actual_correct_by_question_with_answer = v2_fail_rate_2(v2R)\n",
    "\n",
    "    v2_fail_rates = merge_v2_fail_rates(v2_actual_correct_by_question, v2_actual_correct_by_question_with_answer)\n",
    "    \n",
    "    return v2_fail_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REPORT 3 : \"Reading Comprehension\" : rc_question_skill_pass_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rc_fail_rate(rcR):\n",
    "\n",
    "    vR_temp = rcR[['Language', '_worker_id', '_country', 'Fluency', 'Time_Taken_Seconds', '_unit_id', 'title', 'test_',\n",
    "                'question_1_difficulty', 'question_1_google_translate_error', 'Question 1 Skill tested',\n",
    "                'question_2_difficulty', 'question_2_google_translate_error', 'Question 2 Skill tested',\n",
    "                'question_3_difficulty', 'question_3_google_translate_error', 'Question 3 Skill tested',\n",
    "                'question_4_difficulty', 'question_4_google_translate_error', 'Question 4 Skill tested',\n",
    "                'register', 'topic', 'text_type', 'complexity', 'familiarity', \n",
    "                'question_no_1', 'question_no_2', 'question_no_3', 'question_no_4',\n",
    "                'Answer_no_1', 'Answer_no_2', 'Answer_no_3', 'Answer_no_4',\n",
    "                'Score']]\n",
    "    \n",
    "    # evaluate if Answers are the same as the questions. If either Q or A are empty, return NaN\n",
    "    if vR_temp['question_no_1'].isnull().all() == True or vR_temp['Answer_no_1'].isnull().all() == True:      \n",
    "        vR_temp['a1'] = np.nan      \n",
    "    else:   \n",
    "        vR_temp['a1'] = np.where(vR_temp['question_no_1'] == vR_temp['Answer_no_1'], 1, 0).astype('str')\n",
    "        \n",
    "    if vR_temp['question_no_2'].isnull().all() == True or vR_temp['Answer_no_2'].isnull().all() == True:        \n",
    "        vR_temp['a2'] = np.nan      \n",
    "    else:       \n",
    "        vR_temp['a2'] = np.where(vR_temp['question_no_2'] == vR_temp['Answer_no_2'], 1, 0).astype('str')      \n",
    "        \n",
    "    if vR_temp['question_no_3'].isnull().all() == True or vR_temp['Answer_no_3'].isnull().all() == True:  \n",
    "        vR_temp['a3'] = np.nan \n",
    "    else:\n",
    "        vR_temp['a3'] = np.where(vR_temp['question_no_3'] == vR_temp['Answer_no_3'], 1, 0).astype('str')\n",
    "        \n",
    "    if vR_temp['question_no_4'].isnull().all() == True or vR_temp['Answer_no_4'].isnull().all() == True:   \n",
    "        vR_temp['a4'] = np.nan \n",
    "    else:\n",
    "        vR_temp['a4'] = np.where(vR_temp['question_no_4'] == vR_temp['Answer_no_4'], 1, 0).astype('str')\n",
    "    \n",
    "    # Dropping columns\n",
    "    vR_temp = vR_temp.drop(['question_no_1', 'question_no_2', 'question_no_3', 'question_no_4',\n",
    "                            'Answer_no_1', 'Answer_no_2', 'Answer_no_3', 'Answer_no_4', 'Score'], axis =1)  \n",
    "    \n",
    "    # concatenate values from different columns with delimiter ;\n",
    "    vR_temp['Score'] = vR_temp[['a1', 'a2', 'a3', 'a4']].astype('str').agg(';'.join, axis=1) \n",
    "    vR_temp['Question'] = ';'.join(['Question 1', 'Question 2', 'Question 3', 'Question 4'])\n",
    "    vR_temp['Difficulty'] = vR_temp[['question_1_difficulty', 'question_2_difficulty', \n",
    "                                     'question_3_difficulty', 'question_4_difficulty']].astype('str').agg(';'.join, axis=1) \n",
    "    vR_temp['Google_Translate_Error'] = vR_temp[['question_1_google_translate_error', \n",
    "                                                 'question_2_google_translate_error', \n",
    "                                                 'question_3_google_translate_error', \n",
    "                                                 'question_4_google_translate_error']].astype('str').agg(';'.join, axis=1) \n",
    "    vR_temp['Skill'] = vR_temp[['Question 1 Skill tested', 'Question 2 Skill tested', \n",
    "                                'Question 3 Skill tested', 'Question 4 Skill tested']].astype('str').agg(';'.join, axis=1) \n",
    "    \n",
    "    # Dropping more columns\n",
    "    vR_temp = vR_temp.drop(['question_1_difficulty', 'question_1_google_translate_error', 'Question 1 Skill tested', \n",
    "                            'question_2_difficulty', 'question_2_google_translate_error', 'Question 2 Skill tested',\n",
    "                            'question_3_difficulty', 'question_3_google_translate_error', 'Question 3 Skill tested',\n",
    "                            'question_4_difficulty', 'question_4_google_translate_error', 'Question 4 Skill tested',\n",
    "                            'a1', 'a2', 'a3', 'a4'], axis =1)  \n",
    "    \n",
    "    # Python explode function to split delimited columns and expand to rows - row_separate in R\n",
    "    vR_temp =  vR_temp.set_index(['Language', '_worker_id', '_country', 'Fluency', 'Time_Taken_Seconds',\n",
    "       '_unit_id', 'title', 'test_', 'register', 'topic', 'text_type',\n",
    "       'complexity', 'familiarity']).apply(lambda x: x.str.split(';').explode()).reset_index()\n",
    "    \n",
    "    vR_temp[['Score', 'Question', 'Difficulty', 'Google_Translate_Error', 'Skill']] = vR_temp[['Score', 'Question', 'Difficulty', \n",
    "                                                                                               'Google_Translate_Error', 'Skill']].replace('nan', np.nan)\n",
    "    vR_temp = vR_temp.dropna(subset = ['Score'])  # remove rows with NaN values in Score \n",
    "    vR_temp['Score'] = vR_temp['Score'].astype('int') # set Score as integer\n",
    "    \n",
    "    rc_answer = vR_temp\n",
    "    \n",
    "    return vR_temp\n",
    "\n",
    "## Melt RC and categorize question choice with letter and question number\n",
    "def melt_rc_assign(rc_choices, q_list, choice_list):\n",
    "    \n",
    "    df=[]\n",
    "    for ql in q_list:\n",
    "        for cl in choice_list:\n",
    "            df_temp_1 = rc_choices[rc_choices['variable'].str.contains('question_' + str(ql))]\n",
    "            df_temp_2 = df_temp_1[df_temp_1['variable'].str.contains('choice_' + str(cl))]\n",
    "            df_temp_2['Question'] = 'Question ' + str(ql)\n",
    "            if cl == 1 :\n",
    "                df_temp_2['Answer'] = 'a'\n",
    "            elif cl == 2 :\n",
    "                df_temp_2['Answer'] = 'b'\n",
    "            elif cl == 3 :\n",
    "                df_temp_2['Answer'] = 'c'\n",
    "            df.append(df_temp_2)\n",
    "            \n",
    "    rc_choices = pd.concat(df)\n",
    "    return rc_choices\n",
    "\n",
    "## Melt RC and categorize question choice with letter and question number\n",
    "def melt_rc(rcR):\n",
    "\n",
    "    vR_temp = rcR[['Language', '_unit_id', 'title', 'test_',\n",
    "                'question_1_choice_1', 'question_1_choice_2', 'question_1_choice_3',\n",
    "                'question_2_choice_1', 'question_2_choice_2', 'question_2_choice_3',\n",
    "                'question_3_choice_1', 'question_3_choice_2', 'question_3_choice_3',\n",
    "                'question_4_choice_1', 'question_4_choice_2', 'question_4_choice_3']]\n",
    "    \n",
    "    # remove duplicate rows in the dataframe\n",
    "    vR_temp = vR_temp.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    vR_temp = pd.melt(vR_temp, id_vars=['Language', '_unit_id', 'title', 'test_'])\n",
    "    \n",
    "    rc_choices = vR_temp\n",
    "    \n",
    "    q_list, choice_list = [1,2,3,4], [1,2,3]\n",
    "    rc_choices = melt_rc_assign(rc_choices, q_list, choice_list)\n",
    "    rc_choices = rc_choices[['Language', '_unit_id', 'title', 'test_', 'Question', 'Answer', 'variable', 'value']]\n",
    "    rc_choices = rc_choices.sort_values(['Language', 'title', 'test_', 'Question', 'Answer'])\n",
    "    \n",
    "    actual_answer = rc_choices\n",
    "    rater_answer = rc_choices\n",
    "    \n",
    "    return rc_choices, actual_answer, rater_answer\n",
    "\n",
    "# ## Melt RC into long format with actual answers\n",
    "def melt_rc_answer_actual(rcR):\n",
    "    \n",
    "    vR_temp = rcR[['Language', '_worker_id', '_country', 'Fluency', 'Time_Taken_Seconds', '_unit_id', 'title', 'test_',\n",
    "                'question_1_difficulty', 'question_1_google_translate_error', 'Question 1 Skill tested',\n",
    "                'question_2_difficulty', 'question_2_google_translate_error', 'Question 2 Skill tested',\n",
    "                'question_3_difficulty', 'question_3_google_translate_error', 'Question 3 Skill tested',\n",
    "                'question_4_difficulty', 'question_4_google_translate_error', 'Question 4 Skill tested',\n",
    "                'register', 'topic', 'text_type', 'complexity', 'familiarity',\n",
    "                'question_no_1', 'question_no_2', 'question_no_3', 'question_no_4',\n",
    "                'Answer_no_1', 'Answer_no_2', 'Answer_no_3', 'Answer_no_4',\n",
    "                'Score']]\n",
    "    \n",
    "    # evaluate if Answers are the same as the questions. If either Q or A are empty, return NaN\n",
    "    if vR_temp['question_no_1'].isnull().all() == True or vR_temp['Answer_no_1'].isnull().all() == True:      \n",
    "        vR_temp['a1'] = np.nan      \n",
    "    else:   \n",
    "        vR_temp['a1'] = np.where(vR_temp['question_no_1'] == vR_temp['Answer_no_1'], 1, 0).astype('str')\n",
    "        \n",
    "    if vR_temp['question_no_2'].isnull().all() == True or vR_temp['Answer_no_2'].isnull().all() == True:        \n",
    "        vR_temp['a2'] = np.nan      \n",
    "    else:       \n",
    "        vR_temp['a2'] = np.where(vR_temp['question_no_2'] == vR_temp['Answer_no_2'], 1, 0).astype('str')      \n",
    "        \n",
    "    if vR_temp['question_no_3'].isnull().all() == True or vR_temp['Answer_no_3'].isnull().all() == True:  \n",
    "        vR_temp['a3'] = np.nan \n",
    "    else:\n",
    "        vR_temp['a3'] = np.where(vR_temp['question_no_3'] == vR_temp['Answer_no_3'], 1, 0).astype('str')\n",
    "        \n",
    "    if vR_temp['question_no_4'].isnull().all() == True or vR_temp['Answer_no_4'].isnull().all() == True:   \n",
    "        vR_temp['a4'] = np.nan \n",
    "    else:\n",
    "        vR_temp['a4'] = np.where(vR_temp['question_no_4'] == vR_temp['Answer_no_4'], 1, 0).astype('str')\n",
    "    \n",
    "    vR_temp = vR_temp.drop('Score', axis = 1)\n",
    "    \n",
    "    # concatenate values from different columns with delimiter ;\n",
    "    vR_temp['Score'] = vR_temp[['a1', 'a2', 'a3', 'a4']].astype('str').agg(';'.join, axis=1) \n",
    "    vR_temp['Rater_Answer'] = vR_temp[['question_no_1', 'question_no_2', 'question_no_3', 'question_no_4']].astype('str').agg(';'.join, axis=1)\n",
    "    vR_temp['Actual_Answer'] = vR_temp[['Answer_no_1', 'Answer_no_2', 'Answer_no_3', 'Answer_no_4']].astype('str').agg(';'.join, axis=1) \n",
    "    vR_temp['Question'] = ';'.join(['Question 1', 'Question 2', 'Question 3', 'Question 4'])\n",
    "    vR_temp['Difficulty'] = vR_temp[['question_1_difficulty', 'question_2_difficulty', \n",
    "                                     'question_3_difficulty', 'question_4_difficulty']].astype('str').agg(';'.join, axis=1) \n",
    "    vR_temp['Google_Translate_Error'] = vR_temp[['question_1_google_translate_error', \n",
    "                                                 'question_2_google_translate_error', \n",
    "                                                 'question_3_google_translate_error', \n",
    "                                                 'question_4_google_translate_error']].astype('str').agg(';'.join, axis=1) \n",
    "    vR_temp['Skill'] = vR_temp[['Question 1 Skill tested', 'Question 2 Skill tested', \n",
    "                                'Question 3 Skill tested', 'Question 4 Skill tested']].astype('str').agg(';'.join, axis=1) \n",
    "    \n",
    "    vR_temp = vR_temp.drop(['question_1_difficulty', 'question_1_google_translate_error', 'Question 1 Skill tested', \n",
    "                            'question_2_difficulty', 'question_2_google_translate_error', 'Question 2 Skill tested',\n",
    "                            'question_3_difficulty', 'question_3_google_translate_error', 'Question 3 Skill tested',\n",
    "                            'question_4_difficulty', 'question_4_google_translate_error', 'Question 4 Skill tested',\n",
    "                            'question_no_1', 'question_no_2', 'question_no_3', 'question_no_4',\n",
    "                            'Answer_no_1', 'Answer_no_2', 'Answer_no_3', 'Answer_no_4',\n",
    "                            'a1', 'a2', 'a3', 'a4'], axis = 1)\n",
    "    \n",
    "     # Python explode function to split delimited columns and expand to rows - row_separate in R\n",
    "    vR_temp =  vR_temp.set_index(['Language', '_worker_id', '_country', 'Fluency', 'Time_Taken_Seconds',\n",
    "       '_unit_id', 'title', 'test_', 'register', 'topic', 'text_type',\n",
    "       'complexity', 'familiarity']).apply(lambda x: x.str.split(';').explode()).reset_index()\n",
    "    \n",
    "    vR_temp[['Score', 'Rater_Answer', 'Actual_Answer', 'Question', 'Difficulty', 'Google_Translate_Error', 'Skill']] = vR_temp[['Score', 'Rater_Answer', \n",
    "                                                                                                                                'Actual_Answer','Question', \n",
    "                                                                                                                                'Difficulty', \n",
    "                                                                                                                                'Google_Translate_Error', \n",
    "                                                                                                                                'Skill']].replace('nan', np.nan)\n",
    "    vR_temp = vR_temp.dropna(subset = ['Score'])  # remove rows with NaN values in Score \n",
    "    vR_temp['Score'] = vR_temp['Score'].astype('int') # set Score as integer\n",
    "    \n",
    "    rc_answer_actual = vR_temp\n",
    "    \n",
    "    return rc_answer_actual\n",
    "\n",
    "def rc_q_s_pass_rate(rc_answer):\n",
    "    \n",
    "    # first grouping\n",
    "    vR_grouped = rc_answer.groupby(['Language', 'Fluency', '_unit_id', 'title', 'test_', 'Score', 'Question', 'Difficulty', 'register', 'Skill'])['_worker_id'].count().reset_index()\n",
    "    vR_grouped = vR_grouped.rename(columns = {\"_worker_id\" : \"Count\"})\n",
    "    \n",
    "    # second grouping\n",
    "    vR_grouped['Total'] = vR_grouped.groupby(['Language', 'Fluency', '_unit_id', 'title', 'test_', 'Question', 'Difficulty', 'register', 'Skill'])['Count'].transform('sum')   \n",
    "    vR_grouped['Fail_Rate'] = round((vR_grouped['Count'] / vR_grouped['Total']), 2)\n",
    "    \n",
    "    # filter Score 0 \n",
    "    vR_grouped = vR_grouped[vR_grouped['Score'] == 0]\n",
    "    \n",
    "    # sort values by Market and _unit_id \n",
    "    vR_grouped = vR_grouped.sort_values(['Fluency', 'Fail_Rate'], ascending = [True, False])\n",
    "    vR_grouped = vR_grouped.reset_index(drop=True) #re-order df index\n",
    "    \n",
    "    rc_question_skill_pass_rate = vR_grouped\n",
    "    \n",
    "    return rc_question_skill_pass_rate\n",
    "\n",
    "def generate_report_3(rcR):\n",
    "    \n",
    "    rc_answer = rc_fail_rate(rcR)\n",
    "    \n",
    "    rc_choices, actual_answer, rater_answer = melt_rc(rcR)\n",
    "    \n",
    "    rc_answer_actual = melt_rc_answer_actual(rcR)\n",
    "    \n",
    "    rc_question_skill_pass_rate = rc_q_s_pass_rate(rc_answer)\n",
    "    \n",
    "    return rc_question_skill_pass_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REPORT 4 : \"RC with Answers\" : rc_question_skill_pass_rate_answer_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rc_q_s_pass_rate_answer(rc_answer_actual):\n",
    "    \n",
    "    # first grouping\n",
    "    vR_grouped = rc_answer_actual.groupby(['Language', 'Fluency', '_unit_id', 'title', 'test_', 'Actual_Answer', 'Rater_Answer', \n",
    "                                    'Score', 'Question', 'Difficulty', 'register', 'Skill'])['_worker_id'].count().reset_index()\n",
    "    vR_grouped = vR_grouped.rename(columns = {\"_worker_id\" : \"Count\"})\n",
    "    \n",
    "    # second grouping\n",
    "    vR_grouped['Total'] = vR_grouped.groupby(['Language', 'Fluency', '_unit_id', 'title', 'test_', 'Question', 'Difficulty', 'register', 'Skill'])['Count'].transform('sum')   \n",
    "    vR_grouped['Fail_Rate'] = round((vR_grouped['Count'] / vR_grouped['Total']), 2)\n",
    "    \n",
    "    # filter Score 0 \n",
    "    vR_grouped = vR_grouped[vR_grouped['Score'] == 0]\n",
    "    \n",
    "    # sort values by Market and _unit_id \n",
    "    vR_grouped = vR_grouped.sort_values(['Fluency', '_unit_id', 'Question', 'Fail_Rate'], ascending = [True, True, True, False])\n",
    "    vR_grouped = vR_grouped.reset_index(drop=True) #re-order df index\n",
    "    \n",
    "    rc_question_skill_pass_rate_answer = vR_grouped\n",
    "    \n",
    "    return rc_question_skill_pass_rate_answer\n",
    "\n",
    "def join_rc_q_s_pass_rate_answer(rc_question_skill_pass_rate_answer, actual_answer, rater_answer):\n",
    "    \n",
    "    first_join = rc_question_skill_pass_rate_answer\n",
    "    first_join = pd.merge(first_join, actual_answer, how = 'left', \n",
    "                            left_on = [\"Language\", \"_unit_id\", \"title\" , \"test_\", \"Question\", \"Actual_Answer\"],\n",
    "                            right_on = [\"Language\", \"_unit_id\", \"title\" , \"test_\", \"Question\", \"Answer\"])\n",
    "    first_join = first_join.drop('Answer', axis=1)\n",
    "    \n",
    "    second_join = pd.merge(first_join, rater_answer, how = 'left', \n",
    "                            left_on = [\"Language\", \"_unit_id\", \"title\" , \"test_\", \"Question\", \"Rater_Answer\"],\n",
    "                            right_on = [\"Language\", \"_unit_id\", \"title\" , \"test_\", \"Question\", \"Answer\"])\n",
    "    second_join = second_join.drop('Answer', axis=1)\n",
    "    \n",
    "    second_join = second_join[['Language', 'Fluency', '_unit_id', 'title', 'test_', 'Difficulty', 'register', 'Skill', 'Question',\n",
    "                               'Actual_Answer', 'value_x', 'Rater_Answer', 'value_y', 'Count', 'Total', 'Fail_Rate']]\n",
    "  \n",
    "    second_join = second_join.rename(columns = { \"Actual_Answer\" : \"Actual_Answer_Letter\", \n",
    "                                       \"value_x\" : \"Actual_Answer_Text\",\n",
    "                                       \"Rater_Answer\" : \"Rater_Answer_Letter\",\n",
    "                                       \"value_y\" : \"Rater_Answer_Text\"})\n",
    "\n",
    "    rc_question_skill_pass_rate_answer_final = second_join\n",
    "    \n",
    "    return rc_question_skill_pass_rate_answer_final\n",
    "\n",
    "\n",
    "def generate_report_4(rcR):\n",
    "    \n",
    "    rc_choices, actual_answer, rater_answer = melt_rc(rcR)\n",
    "    \n",
    "    rc_answer_actual = melt_rc_answer_actual(rcR)\n",
    "    \n",
    "    rc_question_skill_pass_rate_answer = rc_q_s_pass_rate_answer(rc_answer_actual)\n",
    "\n",
    "    rc_question_skill_pass_rate_answer_final = join_rc_q_s_pass_rate_answer(rc_question_skill_pass_rate_answer, actual_answer, rater_answer)\n",
    "    \n",
    "    return rc_question_skill_pass_rate_answer_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_fail_rate_reports(rcR, v1R, v2R, rc, v1, v2, run_value):\n",
    "    \n",
    "    # Report 1 - Near Exact Match - v1_actual_correct_by_question\n",
    "    v1_actual_correct_by_question =  generate_report_1(v1R)\n",
    "\n",
    "    # Report 2 - Close Match - v2_fail_rates\n",
    "    v2_fail_rates = generate_report_2(v2R)\n",
    "    \n",
    "    # Report 3 - Reading Comprehension - rc_question_skill_pass_rate\n",
    "    rc_question_skill_pass_rate = generate_report_3(rcR)\n",
    "    \n",
    "    # Report 4 - RC with Answers - rc_question_skill_pass_rate_answer_final\n",
    "    rc_question_skill_pass_rate_answer_final = generate_report_4(rcR)\n",
    "    \n",
    "    # store all 4 reports into a dictionary set\n",
    "    list_of_datasets = {\"Near Exact Match\" : v1_actual_correct_by_question,\n",
    "                        \"Close Match\" : v2_fail_rates,\n",
    "                        \"Reading Comprehension\" : rc_question_skill_pass_rate,\n",
    "                        \"RC with Answers\" : rc_question_skill_pass_rate_answer_final}\n",
    "    \n",
    "    if run_value == 'Deployment':\n",
    "    \n",
    "        # store all 3 summaries into a dictionary set\n",
    "        list_of_summaries = {\"deployment_rc\" : rc,\n",
    "                            \"deployment_v1\" : v1,\n",
    "                            \"deployment_v2\" : v2}\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # store all 3 summaries into a dictionary set\n",
    "        list_of_summaries = { run_value + \"_rc\" : rc,\n",
    "                              run_value + \"_v1\" : v1,\n",
    "                              run_value + \"_v2\" : v2}\n",
    "        \n",
    "    \n",
    "    return list_of_datasets, list_of_summaries\n",
    "\n",
    "def file_check_create(root_path, config, language_selected, run_value, run_value_2):\n",
    "    \n",
    "   \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        run_folder = os.path.join(root_path, config['report']['deliverable'], run_value, language_selected)\n",
    "\n",
    "        if not os.path.exists(run_folder):\n",
    "            os.makedirs(run_folder, exist_ok=True)\n",
    "        \n",
    "        folder_tag = 'Deployment Summary'\n",
    "        analysis_folder = os.path.join(root_path, config['report']['analysis'], folder_tag)\n",
    "\n",
    "        if not os.path.exists(analysis_folder):\n",
    "            os.makedirs(analysis_folder, exist_ok=True)\n",
    "            \n",
    "        if not os.path.exists(os.path.join(analysis_folder, 'RC')):\n",
    "            os.makedirs(os.path.join(analysis_folder, 'RC'), exist_ok=True)\n",
    "            \n",
    "        if not os.path.exists(os.path.join(analysis_folder, 'V1')):\n",
    "            os.makedirs(os.path.join(analysis_folder, 'V1'), exist_ok=True)\n",
    "            \n",
    "        if not os.path.exists(os.path.join(analysis_folder, 'V2')):\n",
    "            os.makedirs(os.path.join(analysis_folder, 'V2'), exist_ok=True)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        run_folder = os.path.join(root_path, config['report']['deliverable'], run_value, run_value_2, language_selected)\n",
    "\n",
    "        if not os.path.exists(run_folder):\n",
    "            os.makedirs(run_folder, exist_ok=True)\n",
    "            \n",
    "        folder_tag = 'Grand Summary'\n",
    "        analysis_folder = os.path.join(root_path, config['report']['analysis'], folder_tag)\n",
    "\n",
    "        if not os.path.exists(analysis_folder):\n",
    "            os.makedirs(analysis_folder, exist_ok=True)\n",
    "            \n",
    "        if not os.path.exists(os.path.join(analysis_folder, 'RC')):\n",
    "            os.makedirs(os.path.join(analysis_folder, 'RC'), exist_ok=True)\n",
    "            \n",
    "        if not os.path.exists(os.path.join(analysis_folder, 'V1')):\n",
    "            os.makedirs(os.path.join(analysis_folder, 'V1'), exist_ok=True)\n",
    "            \n",
    "        if not os.path.exists(os.path.join(analysis_folder, 'V2')):\n",
    "            os.makedirs(os.path.join(analysis_folder, 'V2'), exist_ok=True)\n",
    "        \n",
    "    return run_folder, analysis_folder, folder_tag\n",
    "\n",
    "def write_fail_report_to_excel(run_folder, list_of_datasets, encode=None):\n",
    "    \n",
    "    with pd.ExcelWriter(os.path.join(run_folder, 'language_fail_rates.xlsx')) as writer:  \n",
    "        for key, value in list_of_datasets.items():\n",
    "            value.to_excel(writer, sheet_name=key, index=False, encoding=encode)\n",
    "            \n",
    "def write_summary_to_csv(analysis_folder, list_of_summaries, encode=None):\n",
    "    \n",
    "    folders = ['RC', 'V1', 'V2']\n",
    "    for lists, f in zip(list_of_summaries.items(), folders):\n",
    "        key, value = lists[0], lists[1]\n",
    "        value.to_csv(os.path.join(os.path.join(analysis_folder,f), key + '.csv'), index=False, encoding=encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data processing in progress...\n",
      "Initialize data ingestion and file checking...\n",
      "\n",
      "PASS: All files exists!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input the type of run e.g. Deployment, Pilot 1, Pilot 2, Pilot 3 .... etc.:  Pilot 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run type: Pilot 2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input the pilot subfolder name e.g. Pilot 1A, Pilot 2C, Pilot 3A-B .... etc.:  Pilot 2A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pilot subfolder: Pilot 2A\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you know the 'Language' and/or 'Market code' for this file? (y/n) :  y\n",
      "\n",
      "Please enter the Language:  Chinese-Simplified\n",
      "\n",
      "Please enter the Market code: eg. EN-EN for English :  ZH-ZH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting automated data cleaning....\n",
      "\n",
      "Dataframe created from RC file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Missing columns inserted into 'Data' sheet.\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "             Language Market  _worker_id  Score  Percentage Grouping\n",
      "0  Chinese-Simplified  ZH-ZH    45699754     24    1.000000  Pilot 2\n",
      "1  Chinese-Simplified  ZH-ZH    45701425     21    0.875000  Pilot 2\n",
      "2  Chinese-Simplified  ZH-ZH    45746251     24    1.000000  Pilot 2\n",
      "3  Chinese-Simplified  ZH-ZH    45797032     23    0.958333  Pilot 2\n",
      "4  Chinese-Simplified  ZH-ZH    46047420     18    0.750000       GT\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "             Language           _id question_no_1 question_no_2 question_no_3  \\\n",
      "0  Chinese-Simplified  5.765947e+09             b             a             a   \n",
      "1  Chinese-Simplified  5.765995e+09             b             a             a   \n",
      "2  Chinese-Simplified  5.766195e+09             b             a             a   \n",
      "3  Chinese-Simplified  5.766438e+09             b             a             a   \n",
      "4  Chinese-Simplified  5.766751e+09             b             a             a   \n",
      "\n",
      "   question_no_4  question_no_5  \n",
      "0            NaN            NaN  \n",
      "1            NaN            NaN  \n",
      "2            NaN            NaN  \n",
      "3            NaN            NaN  \n",
      "4            NaN            NaN  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading RC raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : RC - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : RC - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : RC - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : RC - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : RC - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : RC - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-7 : RC - Data : checking if columns in the 'Data' sheet are identical to the reference columns ...\n",
      "\u001b[92mPASS\u001b[0m: The columns in the 'Data' sheet are identical to the reference\n",
      "\u001b[1m\n",
      "RC data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_1 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_1 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "             Language Market  _worker_id  Score  Percentage Grouping  \\\n",
      "0  Chinese-Simplified  ZH-ZH    45699754     35       0.875  Pilot 2   \n",
      "1  Chinese-Simplified  ZH-ZH    45701425     34       0.850  Pilot 2   \n",
      "2  Chinese-Simplified  ZH-ZH    45746251     33       0.825  Pilot 2   \n",
      "3  Chinese-Simplified  ZH-ZH    45797032     34       0.850  Pilot 2   \n",
      "4  Chinese-Simplified  ZH-ZH    46009614      0       0.000       GT   \n",
      "\n",
      "                          Unnamed: 4  \n",
      "0                                NaN  \n",
      "1                                NaN  \n",
      "2                                NaN  \n",
      "3                                NaN  \n",
      "4  Michael Grantham - please ignore!  \n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "             Language           _id rater_answer a_domain      a_register  \\\n",
      "0  Chinese-Simplified  5.765851e+09           no   idioms  slang/informal   \n",
      "1  Chinese-Simplified  5.765917e+09           no   idioms  slang/informal   \n",
      "2  Chinese-Simplified  5.765961e+09          yes   idioms  slang/informal   \n",
      "3  Chinese-Simplified  5.766105e+09          yes   idioms  slang/informal   \n",
      "4  Chinese-Simplified  5.766274e+09           no   idioms  slang/informal   \n",
      "\n",
      "  b_domain      b_register  \n",
      "0   idioms  slang/informal  \n",
      "1   idioms  slang/informal  \n",
      "2   idioms  slang/informal  \n",
      "3   idioms  slang/informal  \n",
      "4   idioms  slang/informal  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_1 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_1 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_1 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_1 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_1 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_1 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_1 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_1 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_2 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_2 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "             Language Market  _worker_id  Score  Percentage Grouping\n",
      "0  Chinese-Simplified  ZH-ZH    45699754     61      0.7625  Pilot 2\n",
      "1  Chinese-Simplified  ZH-ZH    45701425     68      0.8500  Pilot 2\n",
      "2  Chinese-Simplified  ZH-ZH    45746251     65      0.8125  Pilot 2\n",
      "3  Chinese-Simplified  ZH-ZH    45797032     66      0.8250  Pilot 2\n",
      "4  Chinese-Simplified  ZH-ZH    46047420     47      0.5875       GT\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "             Language           _id               rater_answer  a_domain  \\\n",
      "0  Chinese-Simplified  5.765925e+09  a_is_more_specific_than_b  aviation   \n",
      "1  Chinese-Simplified  5.765972e+09  a_is_more_specific_than_b  aviation   \n",
      "2  Chinese-Simplified  5.766132e+09  a_is_more_specific_than_b  aviation   \n",
      "3  Chinese-Simplified  5.766307e+09  a_is_more_specific_than_b  aviation   \n",
      "4  Chinese-Simplified  5.766689e+09  a_is_more_specific_than_b  aviation   \n",
      "\n",
      "       a_register  b_domain b_register  \n",
      "0  slang/informal  aviation     formal  \n",
      "1  slang/informal  aviation     formal  \n",
      "2  slang/informal  aviation     formal  \n",
      "3  slang/informal  aviation     formal  \n",
      "4  slang/informal  aviation     formal  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_2 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_2 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_2 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_2 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_2 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_2 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_2 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_2 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Automated data cleaning completed. Cleaned excel files are located in data > processed > Pilot 2 folder. \n",
      "\n",
      "Initialize data ingestion and file checking...\n",
      "\n",
      "\n",
      "\n",
      "     Pilot    Variation\n",
      "0  Pilot 1  Pilot 1A-1B\n",
      "1  Pilot 1     Pilot 1C\n",
      "2  Pilot 1     Pilot 1D\n",
      "3  Pilot 1     Pilot 1E\n",
      "4  Pilot 2     Pilot 2A\n",
      "5  Pilot 2   Pilot 2A-A\n",
      "6  Pilot 2   Pilot 2B-A\n",
      "7  Pilot 2     Pilot 2D\n",
      "8  Pilot 3     Pilot 3A\n",
      "9  Pilot 3   Pilot 3A-A\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select the number of the pilot variation:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have selected 4 for 'Pilot 2A'\n",
      "\n",
      "               Survey Filename\n",
      "0  Survey Pilot 1A and 1B.xlsx\n",
      "1          Survey Pilot 2.xlsx\n",
      "2    Survey Pilot 2 and 3.xlsx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select the number of the survey filename for your pilot run:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have selected 1 for 'Survey Pilot 2.xlsx'\n",
      "\n",
      "Data processing completed.\n",
      "\n",
      "\n",
      "             Language\n",
      "0  Chinese-Simplified\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select the number of the Language you are assessing:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have selected 0 for Chinese-Simplified\n",
      "\n",
      "Generating reports ...\n",
      "\n",
      "1. Language fail rates report completed and stored in reports > deliverables > Pilot 2 > Pilot 2A > Chinese-Simplified\n",
      "\n",
      "2. Summary report completed and stored in analysis > Grand Summary > RC/V1/V2\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    print('\\nData processing in progress...')\n",
    "    # import data from data_processing module\n",
    "    raters, r1, r2, r3, languages, rc, v1, v2, run_value , run_value_2, survey_selected, survey_files = data_processing.main()\n",
    "    print('Data processing completed.')\n",
    "    print(\"\\n\")\n",
    "    print(languages)\n",
    "    \n",
    "    # Get input language selection\n",
    "    language_selected = language_selection(languages)\n",
    "      \n",
    "    # Get data from language modification processes\n",
    "    rcR, v1R, v2R = get_time_taken_all(language_selected, rc, v1, v2)\n",
    "    \n",
    "    print('\\nGenerating reports ...')\n",
    "    \n",
    "    # Start generating fail rate reports\n",
    "    list_of_datasets, list_of_summaries = generate_all_fail_rate_reports(rcR, v1R, v2R, rc, v1, v2, run_value)\n",
    "    \n",
    "    # Check the run type and language and create folders in reports > deliverables\n",
    "    run_folder, analysis_folder, folder_tag = file_check_create(root_path, config, language_selected, run_value, run_value_2)\n",
    "    \n",
    "    # Write reports to excel file in run_folder path\n",
    "    write_fail_report_to_excel(run_folder, list_of_datasets, encode=None)\n",
    "    \n",
    "    print(f\"\\n1. Language fail rates report completed and stored in reports > deliverables > {run_value} > {run_value_2} > {language_selected}\")\n",
    "    \n",
    "    # Write summaries to csv file in analysis_folder path\n",
    "    write_summary_to_csv(analysis_folder, list_of_summaries, encode=None)\n",
    "    \n",
    "    print(f\"\\n2. Summary report completed and stored in analysis > {folder_tag} > RC/V1/V2\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "ala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
