{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################       \n",
    "#Script Name    :                                                                                              \n",
    "#Description    :                                                                                 \n",
    "#Args           :                                                                                           \n",
    "#Author         : Nikhil Rao in R, converted to Python by Nor Raymond                                              \n",
    "#Email          : nraymond@appen.com                                          \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config_path = \"conf/base\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "except:\n",
    "    \n",
    "    os.chdir('..')\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_cleaning module\n",
    "import src.data.data_cleaning as data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to initialize data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_language(data_path, files, file_initials):\n",
    "    \n",
    "    file_groups = {}  \n",
    "    for x in files:  \n",
    "        key = x.split('_')[0] #x[:16] # The key is the first 16 characters of the file name\n",
    "        group = file_groups.get(key,[])\n",
    "        group.append(x)  \n",
    "        file_groups[key] = group\n",
    "                \n",
    "    return file_groups\n",
    "\n",
    "def create_file_exists_df(files, file_initials):\n",
    "    \n",
    "    checker = []\n",
    "    file_exists = []\n",
    "    for fname in files:\n",
    "        for key in file_initials:\n",
    "            if key in fname:\n",
    "                file_exists.append((key, fname))\n",
    "\n",
    "    file_exists = pd.DataFrame(file_exists, columns =['Keyword', 'Filename'])\n",
    "    \n",
    "    return file_exists\n",
    "\n",
    "def data_ingestion_initialize(root_path, run_value, run_value_2):\n",
    "    \n",
    "    # Function to load yaml configuration file\n",
    "    def load_config(config_name):\n",
    "        with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "    \n",
    "    # define reference file paths\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    ref_data = pd.read_excel(io = ref_filepath, sheet_name=\"threshold_raters\", header=None)\n",
    "    \n",
    "    if len(ref_data) != 0:\n",
    "        ref_data_cols = ref_data[0].tolist()\n",
    "    else:\n",
    "        ref_data_cols = []\n",
    "\n",
    "    print(\"Initialize data ingestion and file checking...\\n\")\n",
    "    \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], 'Deployment')\n",
    "        survey_path = ''\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], run_value, run_value_2)\n",
    "        survey_path = os.path.join(root_path, config[\"data_path\"][\"survey\"])\n",
    "        post_survey_path = os.path.join(root_path, config[\"data_path\"][\"post_survey\"])\n",
    "        \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(data_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "    languages = []\n",
    "    for file in files:\n",
    "        for file_initial in file_initials:        \n",
    "            lang = file.split('_' + file_initial)[0]\n",
    "        if not lang.endswith((\".xlsx\")):\n",
    "            languages.append(lang)\n",
    "    \n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    file_groups = group_files_by_language(data_path, files, file_initials)\n",
    "    \n",
    "    file_exists = create_file_exists_df(files, file_initials)\n",
    "    \n",
    "    return data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path, post_survey_path\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - DEPLOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "def obtain_file_summary_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_summary = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Summary')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_summary.append(df)    \n",
    "        \n",
    "    return df_summary\n",
    "\n",
    "def obtain_file_data_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_data = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Data')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_data.append(df)  \n",
    "        \n",
    "    return df_data\n",
    "\n",
    "def obtain_distinct_raters(df_summary, ref_data_cols):\n",
    "\n",
    "    r1 = df_summary[0] # Joined data for Summary sheet from RC \n",
    "    r2 = df_summary[1] # Joined data for Summary page from Vocab_1 \n",
    "    r3 = df_summary[2] # Joined data for Summary page from Vocab_2 \n",
    "             \n",
    "    raters = pd.concat([r1,r2,r3], ignore_index=True)\n",
    "    raters = raters[['_worker_id', 'Grouping', 'Market', 'Language']]\n",
    "    raters = raters.drop_duplicates()\n",
    "    \n",
    "    if len(ref_data_cols) != 0:\n",
    "        \n",
    "        threshold_raters = ref_data_cols\n",
    "        raters = raters[raters['_worker_id'].isin(threshold_raters)]\n",
    "    \n",
    "    # obtain languages from r1 and create a dataframe\n",
    "    languages = r1.Language.unique().tolist()\n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    return raters, r1, r2, r3, languages\n",
    "\n",
    "def merge_raters_to_df_data(df_data, raters):\n",
    "\n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - PILOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pilot_selection(root_path, config):\n",
    "\n",
    "##  read pilot variation data from reference_check.xlsx file\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    pilot_variation = pd.read_excel(io = ref_filepath, sheet_name=\"pilot_variation\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(pilot_variation)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            pilot_index = int(input(\"\\nPlease select the number of the pilot variation: \"))\n",
    "            if pilot_index < min(pilot_variation.index) or pilot_index > max(pilot_variation.index):\n",
    "                print(f\"\\nYou must enter numbers between {min(pilot_variation.index)} - {max(pilot_variation.index)}... Please try again\")\n",
    "                continue\n",
    "            elif pilot_index == \"\":\n",
    "                print(\"\\nYou must enter any numbers\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"\\nYou have selected {pilot_index} for '{pilot_variation.iloc[pilot_index, 1]}'\\n\")\n",
    "                pilot_selected = pilot_variation.iloc[pilot_index, 0]\n",
    "                pilot_var_selected = pilot_variation.iloc[pilot_index, 1]\n",
    "                break\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"\\nYou must enter numerical values only... Please try again\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return pilot_variation, pilot_selected, pilot_var_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_selection(root_path, config):\n",
    "\n",
    "    survey_path = os.path.join(root_path, config[\"data_path\"][\"survey\"])\n",
    "       \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(survey_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    survey_files = pd.DataFrame(files, columns = ['Survey Filename'])\n",
    "    \n",
    "    print(survey_files)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            survey_index = int(input(\"\\nPlease select the number of the survey filename for your pilot run: \"))\n",
    "            if survey_index < min(survey_files.index) or survey_index > max(survey_files.index):\n",
    "                print(f\"\\nYou must enter numbers between {min(survey_files.index)} - {max(survey_files.index)}... Please try again\")\n",
    "                continue\n",
    "            elif survey_index == \"\":\n",
    "                print(\"\\nYou must enter any numbers\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"\\nYou have selected {survey_index} for '{survey_files.iloc[survey_index, 0]}'\\n\")\n",
    "                survey_selected = survey_files.iloc[survey_index, 0]\n",
    "                break\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"\\nYou must enter numerical values only... Please try again\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return survey_selected, survey_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for common pilot operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_survey_fluency(survey_data):\n",
    "    \n",
    "    fluency = []\n",
    "    for opt in survey_data['31_language_1']:\n",
    "        if opt == 'over_15_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '1015_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '510_years' :\n",
    "            fluency.append('Intermediate')\n",
    "        elif opt == '03_years' :\n",
    "            fluency.append('Not Fluent')   \n",
    "        else:\n",
    "            fluency.append('') \n",
    "            \n",
    "    return fluency\n",
    "\n",
    "def obtain_survey_data(survey_path, survey_selected):\n",
    "         \n",
    "    survey_data = pd.read_excel(os.path.join(survey_path, survey_selected), 'Sheet1')\n",
    "    try:\n",
    "        survey_data = survey_data.drop('Unnamed: 42', axis = 1)\n",
    "        survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "        survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "        survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "        survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "    except:\n",
    "        survey_data = survey_data\n",
    "        survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "        survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "        survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "        survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "            \n",
    "    return survey_data\n",
    "\n",
    "def merge_to_survey_data(df_data, raters, survey_data):\n",
    "    \n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Drop duplicate cols\n",
    "    rc = rc.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v1 = v1.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v2 = v2.drop(['Language_y', 'Market'], axis = 1)\n",
    "    \n",
    "    rc = rc.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v1 = v1.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v2 = v2.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    \n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for varied pilot operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and merge to post survey data\n",
    "# Note: functions relevant to Pilot 1A-1B\n",
    "def obtain_post_survey_data(post_survey_path, pilot_selected, pilot_var_selected):\n",
    "\n",
    "    '''\n",
    "    Valid for Pilot 1A-1B\n",
    "    '''\n",
    "        \n",
    "    post_survey_path = os.path.join(root_path, post_survey_path)\n",
    "    post_survey_data = pd.read_excel(os.path.join(post_survey_path, pilot_selected, pilot_var_selected, 'Post Assessment Survey.xlsx'), 'Sheet1')\n",
    "    \n",
    "    postsurvey = post_survey_data[['_worker_id', 'question_1', 'question_1_1', 'question_2', 'question_2_1', 'question_3', 'question_3_1',\n",
    "                                   'question_4', 'question_4_1', 'question_5', 'question_5_1', 'question_6', 'question_6_1']]\n",
    "    \n",
    "    postsurvey.columns = [\"_worker_id\",\n",
    "                          \"Did you feel any questions or content was inappropriate or offensive?\",\n",
    "                          \"If you answered yes, please identify which question? Please explain\",\n",
    "                          \"Did you notice any issues with the language? i.e., grammar, spelling, archaic terminology?\",\n",
    "                          \"If so, what were the issues?\",\n",
    "                          \"Did you feel you had enough time to complete the assessment in one sitting?\",\n",
    "                          \"If so, did you feel pressured for time? Please share your thoughts and experience.\",\n",
    "                          \"Did you feel there was a good range of difficulty in the questions?\",\n",
    "                          \"Please explain your thoughts? Any details you can offer are greatly appreciated.\",\n",
    "                          \"Did you use Google search / Google Translate / Word Reference / Linguee / any other sources to help you complete the test?\",\n",
    "                          \"If so, please list what you used, and explain why you chose to use them?\",\n",
    "                          \"If you did not finish the full assessment, we would love to know why. Did you find it too time consuming? Did you get interrupted?\",\n",
    "                          \"If so, any information you can provide is helpful.\"]\n",
    "    \n",
    "    return postsurvey\n",
    "\n",
    "def obtain_tenure_grouping(dfs, pilot_var_selected):\n",
    "    \n",
    "    '''\n",
    "    Valid for Pilot 1A-1B if Grouping == GT -> Google Translate Control Group\n",
    "    Valid for Pilot 1C - Grouping == GT -> GT\n",
    "    '''\n",
    "       \n",
    "    tenure = []\n",
    "    for opt in dfs['Grouping']:\n",
    "        if opt == 'Pilot 1A' :\n",
    "            tenure.append('90+ days')\n",
    "        elif opt == 'Pilot 1B' :\n",
    "            tenure.append('1 - 90 days')\n",
    "        elif opt == 'GT' :\n",
    "            if pilot_var_selected == 'Pilot 1A-1B':\n",
    "                tenure.append('Google Translate Control Group')\n",
    "            elif pilot_var_selected == 'Pilot 1C':\n",
    "                tenure.append('GT')\n",
    "        else:\n",
    "            tenure.append('') \n",
    "            \n",
    "    return tenure\n",
    "\n",
    "def merge_to_post_survey_data(rc, v1, v2, postsurvey):\n",
    "    \n",
    "    '''\n",
    "    Valid for Pilot 1A-1B\n",
    "    '''\n",
    "    \n",
    "    # Merge postsurvey data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, postsurvey,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, postsurvey,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, postsurvey,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    rc, v1, v2 = fluency_categorization_group_3(rc, v1, v2)\n",
    "    \n",
    "        \n",
    "    rc['Tenure'] = obtain_tenure_grouping(rc, pilot_var_selected)\n",
    "    v1['Tenure'] = obtain_tenure_grouping(v1, pilot_var_selected)    \n",
    "    v2['Tenure'] = obtain_tenure_grouping(v2, pilot_var_selected)    \n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "\n",
    "# Fluency categorization functions\n",
    "# Note: missing group 3A-A , 2A-A has to be manual\n",
    "\n",
    "def fluency_categorization_group_1(rc, v1, v2):\n",
    "    \n",
    "    '''\n",
    "    Valid for 1C , 1D , 2A\n",
    "    '''\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    rc = rc.dropna(subset=['Fluency'])\n",
    "    v1 = v1.dropna(subset=['Fluency'])\n",
    "    v2 = v2.dropna(subset=['Fluency'])\n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "def fluency_categorization_group_2(rc, v1, v2):\n",
    "    \n",
    "    '''\n",
    "    Valid for 1E , 2D , 2B-A , 3A\n",
    "    '''\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Fluency'].isna(), 'Fluent', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Fluency'].isna(), 'Fluent', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Fluency'].isna(), 'Fluent', v2['Fluency'])\n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "def fluency_categorization_group_3(rc, v1, v2):\n",
    "    \n",
    "    '''\n",
    "    Valid for 1A-1B\n",
    "    '''\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['31_language_1'].isna(), 'Not Categorized', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['31_language_1'].isna(), 'Not Categorized', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['31_language_1'].isna(), 'Not Categorized', v2['Fluency'])\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize data ingestion and file checking...\n",
      "\n",
      "PASS: All files exists!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input the type of run e.g. Deployment, Pilot 1, Pilot 2, Pilot 3 .... etc.:  Pilot 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run type: Pilot 2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input the pilot subfolder name e.g. Pilot 1A, Pilot 2C, Pilot 3A-B .... etc.:  Pilot 2A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pilot subfolder: Pilot 2A\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you know the 'Language' and/or 'Market code' for this file? (y/n) :  y\n",
      "\n",
      "Please enter the Language:  Chinese-Simplified\n",
      "\n",
      "Please enter the Market code: eg. EN-EN for English :  ZH-ZH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting automated data cleaning....\n",
      "\n",
      "Dataframe created from RC file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Missing columns inserted into 'Data' sheet.\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "             Language Market  _worker_id  Score  Percentage Grouping\n",
      "0  Chinese-Simplified  ZH-ZH    45699754     24    1.000000  Pilot 2\n",
      "1  Chinese-Simplified  ZH-ZH    45701425     21    0.875000  Pilot 2\n",
      "2  Chinese-Simplified  ZH-ZH    45746251     24    1.000000  Pilot 2\n",
      "3  Chinese-Simplified  ZH-ZH    45797032     23    0.958333  Pilot 2\n",
      "4  Chinese-Simplified  ZH-ZH    46047420     18    0.750000       GT\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "             Language           _id question_no_1 question_no_2 question_no_3  \\\n",
      "0  Chinese-Simplified  5.765947e+09             b             a             a   \n",
      "1  Chinese-Simplified  5.765995e+09             b             a             a   \n",
      "2  Chinese-Simplified  5.766195e+09             b             a             a   \n",
      "3  Chinese-Simplified  5.766438e+09             b             a             a   \n",
      "4  Chinese-Simplified  5.766751e+09             b             a             a   \n",
      "\n",
      "   question_no_4  question_no_5  \n",
      "0            NaN            NaN  \n",
      "1            NaN            NaN  \n",
      "2            NaN            NaN  \n",
      "3            NaN            NaN  \n",
      "4            NaN            NaN  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading RC raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : RC - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : RC - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : RC - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : RC - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : RC - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : RC - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-7 : RC - Data : checking if columns in the 'Data' sheet are identical to the reference columns ...\n",
      "\u001b[92mPASS\u001b[0m: The columns in the 'Data' sheet are identical to the reference\n",
      "\u001b[1m\n",
      "RC data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_1 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_1 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "             Language Market  _worker_id  Score  Percentage Grouping  \\\n",
      "0  Chinese-Simplified  ZH-ZH    45699754     35       0.875  Pilot 2   \n",
      "1  Chinese-Simplified  ZH-ZH    45701425     34       0.850  Pilot 2   \n",
      "2  Chinese-Simplified  ZH-ZH    45746251     33       0.825  Pilot 2   \n",
      "3  Chinese-Simplified  ZH-ZH    45797032     34       0.850  Pilot 2   \n",
      "4  Chinese-Simplified  ZH-ZH    46009614      0       0.000       GT   \n",
      "\n",
      "                          Unnamed: 4  \n",
      "0                                NaN  \n",
      "1                                NaN  \n",
      "2                                NaN  \n",
      "3                                NaN  \n",
      "4  Michael Grantham - please ignore!  \n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "             Language           _id rater_answer a_domain      a_register  \\\n",
      "0  Chinese-Simplified  5.765851e+09           no   idioms  slang/informal   \n",
      "1  Chinese-Simplified  5.765917e+09           no   idioms  slang/informal   \n",
      "2  Chinese-Simplified  5.765961e+09          yes   idioms  slang/informal   \n",
      "3  Chinese-Simplified  5.766105e+09          yes   idioms  slang/informal   \n",
      "4  Chinese-Simplified  5.766274e+09           no   idioms  slang/informal   \n",
      "\n",
      "  b_domain      b_register  \n",
      "0   idioms  slang/informal  \n",
      "1   idioms  slang/informal  \n",
      "2   idioms  slang/informal  \n",
      "3   idioms  slang/informal  \n",
      "4   idioms  slang/informal  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_1 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_1 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_1 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_1 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_1 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_1 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_1 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_1 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_2 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_2 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "             Language Market  _worker_id  Score  Percentage Grouping\n",
      "0  Chinese-Simplified  ZH-ZH    45699754     61      0.7625  Pilot 2\n",
      "1  Chinese-Simplified  ZH-ZH    45701425     68      0.8500  Pilot 2\n",
      "2  Chinese-Simplified  ZH-ZH    45746251     65      0.8125  Pilot 2\n",
      "3  Chinese-Simplified  ZH-ZH    45797032     66      0.8250  Pilot 2\n",
      "4  Chinese-Simplified  ZH-ZH    46047420     47      0.5875       GT\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "             Language           _id               rater_answer  a_domain  \\\n",
      "0  Chinese-Simplified  5.765925e+09  a_is_more_specific_than_b  aviation   \n",
      "1  Chinese-Simplified  5.765972e+09  a_is_more_specific_than_b  aviation   \n",
      "2  Chinese-Simplified  5.766132e+09  a_is_more_specific_than_b  aviation   \n",
      "3  Chinese-Simplified  5.766307e+09  a_is_more_specific_than_b  aviation   \n",
      "4  Chinese-Simplified  5.766689e+09  a_is_more_specific_than_b  aviation   \n",
      "\n",
      "       a_register  b_domain b_register  \n",
      "0  slang/informal  aviation     formal  \n",
      "1  slang/informal  aviation     formal  \n",
      "2  slang/informal  aviation     formal  \n",
      "3  slang/informal  aviation     formal  \n",
      "4  slang/informal  aviation     formal  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_2 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_2 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_2 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_2 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_2 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_2 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_2 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_2 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Automated data cleaning completed. Cleaned excel files are located in data > processed > Pilot 2 folder. \n",
      "\n",
      "Initialize data ingestion and file checking...\n",
      "\n",
      "\n",
      "\n",
      "     Pilot    Variation\n",
      "0  Pilot 1  Pilot 1A-1B\n",
      "1  Pilot 1     Pilot 1C\n",
      "2  Pilot 1     Pilot 1D\n",
      "3  Pilot 1     Pilot 1E\n",
      "4  Pilot 2     Pilot 2A\n",
      "5  Pilot 2   Pilot 2A-A\n",
      "6  Pilot 2   Pilot 2B-A\n",
      "7  Pilot 2     Pilot 2D\n",
      "8  Pilot 3     Pilot 3A\n",
      "9  Pilot 3   Pilot 3A-A\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select the number of the pilot variation:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have selected 4 for 'Pilot 2A'\n",
      "\n",
      "               Survey Filename\n",
      "0  Survey Pilot 1A and 1B.xlsx\n",
      "1          Survey Pilot 2.xlsx\n",
      "2    Survey Pilot 2 and 3.xlsx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select the number of the survey filename for your pilot run:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have selected 1 for 'Survey Pilot 2.xlsx'\n",
      "\n",
      "\n",
      "Automated data processing completed.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "    \n",
    "    language, market, run_value, run_value_2  = data_cleaning.main()\n",
    "    \n",
    "    data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path, post_survey_path = data_ingestion_initialize(root_path, run_value, run_value_2)\n",
    "    \n",
    "    df_summary = obtain_file_summary_df(file_initials, file_exists, data_path)\n",
    "    df_data = obtain_file_data_df(file_initials, file_exists, data_path)\n",
    "    raters, r1, r2, r3, languages =  obtain_distinct_raters(df_summary, ref_data_cols)\n",
    "        \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        rc, v1, v2 = merge_raters_to_df_data(df_data, raters)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        pilot_variation, pilot_selected, pilot_var_selected = pilot_selection(root_path, config)\n",
    "        survey_selected, survey_files = survey_selection(root_path, config)\n",
    "        survey_data = obtain_survey_data(survey_path, survey_selected)\n",
    "        rc, v1, v2 = merge_to_survey_data(df_data, raters, survey_data)\n",
    "        \n",
    "        if pilot_var_selected == 'Pilot 1A-1B':\n",
    "            \n",
    "            postsurvey = obtain_post_survey_data(post_survey_path, pilot_selected, pilot_var_selected)\n",
    "            rc, v1, v2 = merge_to_post_survey_data(rc, v1, v2, postsurvey)\n",
    "            \n",
    "        elif pilot_var_selected == 'Pilot 1C':\n",
    "            \n",
    "            rc, v1, v2 = fluency_categorization_group_1(rc, v1, v2)\n",
    "            rc['Tenure'] = obtain_tenure_grouping(rc, pilot_var_selected)\n",
    "            v1['Tenure'] = obtain_tenure_grouping(v1, pilot_var_selected)    \n",
    "            v2['Tenure'] = obtain_tenure_grouping(v2, pilot_var_selected) \n",
    "            \n",
    "        elif pilot_var_selected == 'Pilot 1D' or pilot_var_selected == 'Pilot 2A':\n",
    "            \n",
    "            rc, v1, v2 = fluency_categorization_group_1(rc, v1, v2)    \n",
    "            \n",
    "        elif pilot_var_selected == 'Pilot 1E':\n",
    "            \n",
    "            rc, v1, v2 = fluency_categorization_group_2(rc, v1, v2)   \n",
    " \n",
    "    return rc, v1, v2, survey_data, r1, r2, r3\n",
    "\n",
    "#    return raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2, survey_selected, survey_files  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2, survey_selected, survey_files  = main()\n",
    "#     print(languages)\n",
    "\n",
    "    rc, v1, v2, survey_data, r1, r2, r3 = main()\n",
    "    \n",
    "    print('\\nAutomated data processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>_id</th>\n",
       "      <th>rater_answer</th>\n",
       "      <th>a_domain</th>\n",
       "      <th>a_register</th>\n",
       "      <th>b_domain</th>\n",
       "      <th>b_register</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>familiarity</th>\n",
       "      <th>question_</th>\n",
       "      <th>...</th>\n",
       "      <th>_ip</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Alternate Answer</th>\n",
       "      <th>Score</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Grouping</th>\n",
       "      <th>31_language_1</th>\n",
       "      <th>survey_created_at</th>\n",
       "      <th>survey_started_at</th>\n",
       "      <th>Fluency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5765924548</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>aviation</td>\n",
       "      <td>slang/informal</td>\n",
       "      <td>aviation</td>\n",
       "      <td>formal</td>\n",
       "      <td>hard</td>\n",
       "      <td>unfamiliar</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>45.135.186.84</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a_is_more_specific_than_b;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-07 01:00:39</td>\n",
       "      <td>2020-10-07 00:59:07</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5765972394</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>aviation</td>\n",
       "      <td>slang/informal</td>\n",
       "      <td>aviation</td>\n",
       "      <td>formal</td>\n",
       "      <td>hard</td>\n",
       "      <td>unfamiliar</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>103.136.251.48</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a_is_more_specific_than_b;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-07 11:54:18</td>\n",
       "      <td>2020-10-07 11:49:30</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5766132493</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>aviation</td>\n",
       "      <td>slang/informal</td>\n",
       "      <td>aviation</td>\n",
       "      <td>formal</td>\n",
       "      <td>hard</td>\n",
       "      <td>unfamiliar</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>114.240.58.121</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a_is_more_specific_than_b;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-07 08:06:48</td>\n",
       "      <td>2020-10-07 08:03:12</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5766307171</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>aviation</td>\n",
       "      <td>slang/informal</td>\n",
       "      <td>aviation</td>\n",
       "      <td>formal</td>\n",
       "      <td>hard</td>\n",
       "      <td>unfamiliar</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>210.3.145.194</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a_is_more_specific_than_b;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-07 09:00:43</td>\n",
       "      <td>2020-10-07 08:56:00</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5766689403</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>aviation</td>\n",
       "      <td>slang/informal</td>\n",
       "      <td>aviation</td>\n",
       "      <td>formal</td>\n",
       "      <td>hard</td>\n",
       "      <td>unfamiliar</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>91.220.202.202</td>\n",
       "      <td>a_is_more_specific_than_b</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a_is_more_specific_than_b;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-07 01:10:43</td>\n",
       "      <td>2020-10-07 01:06:33</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5766777381</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>familiar</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>104.163.191.52</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>b_is_more_specific_than_a;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-07 15:57:37</td>\n",
       "      <td>2020-10-07 15:54:09</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5767073413</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>familiar</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>38.94.109.21</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>b_is_more_specific_than_a;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-06 21:49:40</td>\n",
       "      <td>2020-10-06 21:48:15</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5769869916</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>familiar</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>49.144.1.167</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>b_is_more_specific_than_a;0</td>\n",
       "      <td>GT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5769891239</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>familiar</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>99.150.146.137</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>b_is_more_specific_than_a;0</td>\n",
       "      <td>GT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>Chinese-Simplified</td>\n",
       "      <td>5771528967</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>music</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>familiar</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>112.193.10.250</td>\n",
       "      <td>b_is_more_specific_than_a</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>b_is_more_specific_than_a;0</td>\n",
       "      <td>Pilot 2</td>\n",
       "      <td>over_15_years</td>\n",
       "      <td>2020-10-11 09:36:58</td>\n",
       "      <td>2020-10-11 09:27:10</td>\n",
       "      <td>Fluent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Language         _id               rater_answer  a_domain  \\\n",
       "0    Chinese-Simplified  5765924548  a_is_more_specific_than_b  aviation   \n",
       "1    Chinese-Simplified  5765972394  a_is_more_specific_than_b  aviation   \n",
       "2    Chinese-Simplified  5766132493  a_is_more_specific_than_b  aviation   \n",
       "3    Chinese-Simplified  5766307171  a_is_more_specific_than_b  aviation   \n",
       "4    Chinese-Simplified  5766689403  a_is_more_specific_than_b  aviation   \n",
       "..                  ...         ...                        ...       ...   \n",
       "875  Chinese-Simplified  5766777381  b_is_more_specific_than_a     music   \n",
       "876  Chinese-Simplified  5767073413  b_is_more_specific_than_a     music   \n",
       "877  Chinese-Simplified  5769869916  b_is_more_specific_than_a     music   \n",
       "878  Chinese-Simplified  5769891239  b_is_more_specific_than_a     music   \n",
       "879  Chinese-Simplified  5771528967  b_is_more_specific_than_a     music   \n",
       "\n",
       "         a_register  b_domain b_register difficulty familiarity  question_  \\\n",
       "0    slang/informal  aviation     formal       hard  unfamiliar          1   \n",
       "1    slang/informal  aviation     formal       hard  unfamiliar          1   \n",
       "2    slang/informal  aviation     formal       hard  unfamiliar          1   \n",
       "3    slang/informal  aviation     formal       hard  unfamiliar          1   \n",
       "4    slang/informal  aviation     formal       hard  unfamiliar          1   \n",
       "..              ...       ...        ...        ...         ...        ...   \n",
       "875         neutral     music    neutral     medium    familiar         80   \n",
       "876         neutral     music    neutral     medium    familiar         80   \n",
       "877         neutral     music    neutral     medium    familiar         80   \n",
       "878         neutral     music    neutral     medium    familiar         80   \n",
       "879         neutral     music    neutral     medium    familiar         80   \n",
       "\n",
       "     ...             _ip                     Answer Alternate Answer Score  \\\n",
       "0    ...   45.135.186.84  a_is_more_specific_than_b                0     1   \n",
       "1    ...  103.136.251.48  a_is_more_specific_than_b                0     1   \n",
       "2    ...  114.240.58.121  a_is_more_specific_than_b                0     1   \n",
       "3    ...   210.3.145.194  a_is_more_specific_than_b                0     1   \n",
       "4    ...  91.220.202.202  a_is_more_specific_than_b                0     1   \n",
       "..   ...             ...                        ...              ...   ...   \n",
       "875  ...  104.163.191.52  b_is_more_specific_than_a                0     1   \n",
       "876  ...    38.94.109.21  b_is_more_specific_than_a                0     1   \n",
       "877  ...    49.144.1.167  b_is_more_specific_than_a                0     1   \n",
       "878  ...  99.150.146.137  b_is_more_specific_than_a                0     1   \n",
       "879  ...  112.193.10.250  b_is_more_specific_than_a                0     1   \n",
       "\n",
       "                         Answers  Grouping  31_language_1   survey_created_at  \\\n",
       "0    a_is_more_specific_than_b;0   Pilot 2  over_15_years 2020-10-07 01:00:39   \n",
       "1    a_is_more_specific_than_b;0   Pilot 2  over_15_years 2020-10-07 11:54:18   \n",
       "2    a_is_more_specific_than_b;0   Pilot 2  over_15_years 2020-10-07 08:06:48   \n",
       "3    a_is_more_specific_than_b;0   Pilot 2  over_15_years 2020-10-07 09:00:43   \n",
       "4    a_is_more_specific_than_b;0   Pilot 2  over_15_years 2020-10-07 01:10:43   \n",
       "..                           ...       ...            ...                 ...   \n",
       "875  b_is_more_specific_than_a;0   Pilot 2  over_15_years 2020-10-07 15:57:37   \n",
       "876  b_is_more_specific_than_a;0   Pilot 2  over_15_years 2020-10-06 21:49:40   \n",
       "877  b_is_more_specific_than_a;0        GT            NaN                 NaT   \n",
       "878  b_is_more_specific_than_a;0        GT            NaN                 NaT   \n",
       "879  b_is_more_specific_than_a;0   Pilot 2  over_15_years 2020-10-11 09:36:58   \n",
       "\n",
       "      survey_started_at Fluency  \n",
       "0   2020-10-07 00:59:07  Fluent  \n",
       "1   2020-10-07 11:49:30  Fluent  \n",
       "2   2020-10-07 08:03:12  Fluent  \n",
       "3   2020-10-07 08:56:00  Fluent  \n",
       "4   2020-10-07 01:06:33  Fluent  \n",
       "..                  ...     ...  \n",
       "875 2020-10-07 15:54:09  Fluent  \n",
       "876 2020-10-06 21:48:15  Fluent  \n",
       "877                 NaT      GT  \n",
       "878                 NaT      GT  \n",
       "879 2020-10-11 09:27:10  Fluent  \n",
       "\n",
       "[880 rows x 35 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  create manual pilot variation data    \n",
    "#     data = {'Pilot': ['Pilot 1', 'Pilot 1', 'Pilot 1', 'Pilot 1', 'Pilot 2', 'Pilot 2','Pilot 2', 'Pilot 2', 'Pilot 3', 'Pilot 3'],\n",
    "#             'Variation': ['Pilot 1A-1B', 'Pilot 1C', 'Pilot 1D', 'Pilot 1E', 'Pilot 2A', 'Pilot 2A-A', 'Pilot 2B-A', 'Pilot 2D', 'Pilot 3A', 'Pilot 3A-A']}\n",
    "    \n",
    "#     pilot_variation = pd.DataFrame.from_records(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "ala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
