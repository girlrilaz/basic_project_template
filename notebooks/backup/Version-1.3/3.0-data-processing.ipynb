{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################       \n",
    "#Script Name    :                                                                                              \n",
    "#Description    :                                                                                 \n",
    "#Args           :                                                                                           \n",
    "#Author         : Nikhil Rao in R, converted to Python by Nor Raymond                                              \n",
    "#Email          : nraymond@appen.com                                          \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config_path = \"conf/base\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "except:\n",
    "    \n",
    "    os.chdir('..')\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_cleaning module\n",
    "import src.data.data_cleaning as data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to initialize data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_language(data_path, files, file_initials):\n",
    "    \n",
    "    file_groups = {}  \n",
    "    for x in files:  \n",
    "        key = x.split('_')[0] #x[:16] # The key is the first 16 characters of the file name\n",
    "        group = file_groups.get(key,[])\n",
    "        group.append(x)  \n",
    "        file_groups[key] = group\n",
    "                \n",
    "    return file_groups\n",
    "\n",
    "def create_file_exists_df(files, file_initials):\n",
    "    \n",
    "    checker = []\n",
    "    file_exists = []\n",
    "    for fname in files:\n",
    "        for key in file_initials:\n",
    "            if key in fname:\n",
    "                file_exists.append((key, fname))\n",
    "\n",
    "    file_exists = pd.DataFrame(file_exists, columns =['Keyword', 'Filename'])\n",
    "    \n",
    "    return file_exists\n",
    "\n",
    "def data_ingestion_initialize(root_path, run_value, run_value_2):\n",
    "    \n",
    "    # Function to load yaml configuration file\n",
    "    def load_config(config_name):\n",
    "        with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "    \n",
    "    # define reference file paths\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    ref_data = pd.read_excel(io = ref_filepath, sheet_name=\"threshold_raters\", header=None)\n",
    "    \n",
    "    if len(ref_data) != 0:\n",
    "        ref_data_cols = ref_data[0].tolist()\n",
    "    else:\n",
    "        ref_data_cols = []\n",
    "\n",
    "    print(\"Initialize data ingestion and file checking...\\n\")\n",
    "    \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], 'Deployment')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], run_value, run_value_2)\n",
    "        \n",
    "       \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(data_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "    languages = []\n",
    "    for file in files:\n",
    "        for file_initial in file_initials:        \n",
    "            lang = file.split('_' + file_initial)[0]\n",
    "        if not lang.endswith((\".xlsx\")):\n",
    "            languages.append(lang)\n",
    "    \n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    file_groups = group_files_by_language(data_path, files, file_initials)\n",
    "    \n",
    "    file_exists = create_file_exists_df(files, file_initials)\n",
    "    \n",
    "    return data_path, files, languages, file_groups, file_exists, ref_data_cols\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "def obtain_file_summary_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_summary = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Summary')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_summary.append(df)    \n",
    "        \n",
    "    return df_summary\n",
    "\n",
    "def obtain_file_data_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_data = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Data')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_data.append(df)    \n",
    "        \n",
    "    return df_data\n",
    "\n",
    "def obtain_distinct_raters(df_summary, ref_data_cols):\n",
    "\n",
    "    r1 = df_summary[0] # Joined data for Summary sheet from RC \n",
    "    r2 = df_summary[1] # Joined data for Summary page from Vocab_1 \n",
    "    r3 = df_summary[2] # Joined data for Summary page from Vocab_2 \n",
    "             \n",
    "    raters = pd.concat([r1,r2,r3], ignore_index=True)\n",
    "    raters = raters[['_worker_id', 'Grouping', 'Market', 'Language']]\n",
    "    raters = raters.drop_duplicates()\n",
    "    \n",
    "    if len(ref_data_cols) != 0:\n",
    "        \n",
    "        threshold_raters = ref_data_cols\n",
    "        raters = raters[raters['_worker_id'].isin(threshold_raters)]\n",
    "    \n",
    "    # obtain languages from r1 and create a dataframe\n",
    "    languages = r1.Language.unique().tolist()\n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    return raters, r1, r2, r3, languages\n",
    "\n",
    "def merge_raters_to_df_data(df_data, raters):\n",
    "\n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize data ingestion and file checking...\n",
      "\n",
      "PASS: All files exists!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input the type of run e.g. Deployment, Pilot 1, Pilot 2, Pilot 3 .... etc.:  Deployment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run type: Deployment\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you know the 'Language' and/or 'Market code' for this file? (y/n) :  y\n",
      "\n",
      "Please enter the Language:  Turkish\n",
      "\n",
      "Please enter the Market code: eg. EN-EN for English :  TR-TR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting automated data cleaning....\n",
      "\n",
      "Dataframe created from RC file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Column name worker_id replaced with _worker_id\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Missing columns inserted into 'Data' sheet.\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "  Language Market  _worker_id  Score  Percentage Grouping\n",
      "0  Turkish  TR-TR    44653035     11    0.916667       3A\n",
      "1  Turkish  TR-TR    45488677     11    0.916667       3A\n",
      "2  Turkish  TR-TR    45488685     10    0.833333       3A\n",
      "3  Turkish  TR-TR    45488743     11    0.916667       3A\n",
      "4  Turkish  TR-TR    45488787     12    1.000000       3A\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "  Language         _id question_no_1 question_no_2 question_no_3  \\\n",
      "0  Turkish  5902352949             b             c             c   \n",
      "1  Turkish  5902359926             b             c             b   \n",
      "2  Turkish  5902370632             b             c             c   \n",
      "3  Turkish  5902376208             b             c             c   \n",
      "4  Turkish  5902377642             b             c             c   \n",
      "\n",
      "   question_no_4  question_no_5  \n",
      "0            NaN            NaN  \n",
      "1            NaN            NaN  \n",
      "2            NaN            NaN  \n",
      "3            NaN            NaN  \n",
      "4            NaN            NaN  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading RC raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : RC - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : RC - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : RC - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : RC - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : RC - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : RC - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-7 : RC - Data : checking if columns in the 'Data' sheet are identical to the reference columns ...\n",
      "\u001b[92mPASS\u001b[0m: The columns in the 'Data' sheet are identical to the reference\n",
      "\u001b[1m\n",
      "RC data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_1 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Column name worker_id replaced with _worker_id\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_1 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "  Language Market  _worker_id  Score  Percentage Grouping\n",
      "0  Turkish  TR-TR    44653035     19        0.95       3A\n",
      "1  Turkish  TR-TR    45488677     20        1.00       3A\n",
      "2  Turkish  TR-TR    45488685     20        1.00       3A\n",
      "3  Turkish  TR-TR    45488743     20        1.00       3A\n",
      "4  Turkish  TR-TR    45488787     18        0.90       3A\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "  Language         _id rater_answer    a_domain a_register    b_domain  \\\n",
      "0  Turkish  5902316914          yes  literature     formal  literature   \n",
      "1  Turkish  5902329904          yes  literature     formal  literature   \n",
      "2  Turkish  5902332581          yes  literature     formal  literature   \n",
      "3  Turkish  5902332827          yes  literature     formal  literature   \n",
      "4  Turkish  5902334135          yes  literature     formal  literature   \n",
      "\n",
      "  b_register  \n",
      "0    neutral  \n",
      "1    neutral  \n",
      "2    neutral  \n",
      "3    neutral  \n",
      "4    neutral  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_1 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_1 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_1 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_1 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_1 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_1 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_1 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_1 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_2 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_2 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "  Language Market  _worker_id  Score  Percentage Grouping\n",
      "0  Turkish  TR-TR    44653035     38       0.950       3A\n",
      "1  Turkish  TR-TR    45488677     37       0.925       3A\n",
      "2  Turkish  TR-TR    45488685     33       0.825       3A\n",
      "3  Turkish  TR-TR    45488743     39       0.975       3A\n",
      "4  Turkish  TR-TR    45488787     37       0.925       3A\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "  Language         _id                   rater_answer a_domain a_register  \\\n",
      "0  Turkish  5902331505  a_and_b_have_the_same_meaning     time     formal   \n",
      "1  Turkish  5902341500  a_and_b_have_the_same_meaning     time     formal   \n",
      "2  Turkish  5902345264  a_and_b_have_the_same_meaning     time     formal   \n",
      "3  Turkish  5902349234  a_and_b_have_the_same_meaning     time     formal   \n",
      "4  Turkish  5902352948  a_and_b_have_the_same_meaning     time     formal   \n",
      "\n",
      "  b_domain b_register  \n",
      "0     time    neutral  \n",
      "1     time    neutral  \n",
      "2     time    neutral  \n",
      "3     time    neutral  \n",
      "4     time    neutral  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_2 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_2 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_2 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_2 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_2 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_2 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_2 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_2 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Automated data cleaning completed. Cleaned excel files are located in data > processed > Deployment folder. \n",
      "\n",
      "Initialize data ingestion and file checking...\n",
      "\n",
      "              Language\n",
      "0              Russian\n",
      "1               Hebrew\n",
      "2           Indonesian\n",
      "3              Turkish\n",
      "4  Chinese(Simplified)\n",
      "\n",
      "Automated data processing completed.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "    \n",
    "    language, market, run_value, run_value_2  = data_cleaning.main()\n",
    "    \n",
    "    data_path, files, languages, file_groups, file_exists, ref_data_cols = data_ingestion_initialize(root_path, run_value, run_value_2)\n",
    "    \n",
    "    df_summary = obtain_file_summary_df(file_initials, file_exists, data_path)\n",
    "    df_data = obtain_file_data_df(file_initials, file_exists, data_path)\n",
    "    raters, r1, r2, r3, languages =  obtain_distinct_raters(df_summary, ref_data_cols)\n",
    "    rc, v1, v2 = merge_raters_to_df_data(df_data, raters)\n",
    "    \n",
    "    return raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2 \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2  = main()\n",
    "    print(languages)\n",
    "    print('\\nAutomated data processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "ala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
