{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################       \n",
    "#Script Name    :                                                                                              \n",
    "#Description    :                                                                                 \n",
    "#Args           :                                                                                           \n",
    "#Author         : Nikhil Rao in R, converted to Python by Nor Raymond                                              \n",
    "#Email          : nraymond@appen.com                                          \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config_path = \"conf/base\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "except:\n",
    "    \n",
    "    os.chdir('..')\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_cleaning module\n",
    "import src.data.data_cleaning as data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to initialize data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_language(data_path, files, file_initials):\n",
    "    \n",
    "    file_groups = {}  \n",
    "    for x in files:  \n",
    "        key = x.split('_')[0] #x[:16] # The key is the first 16 characters of the file name\n",
    "        group = file_groups.get(key,[])\n",
    "        group.append(x)  \n",
    "        file_groups[key] = group\n",
    "                \n",
    "    return file_groups\n",
    "\n",
    "def create_file_exists_df(files, file_initials):\n",
    "    \n",
    "    checker = []\n",
    "    file_exists = []\n",
    "    for fname in files:\n",
    "        for key in file_initials:\n",
    "            if key in fname:\n",
    "                file_exists.append((key, fname))\n",
    "\n",
    "    file_exists = pd.DataFrame(file_exists, columns =['Keyword', 'Filename'])\n",
    "    \n",
    "    return file_exists\n",
    "\n",
    "def data_ingestion_initialize(root_path, run_value, run_value_2):\n",
    "    \n",
    "    # Function to load yaml configuration file\n",
    "    def load_config(config_name):\n",
    "        with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "    \n",
    "    # define reference file paths\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    ref_data = pd.read_excel(io = ref_filepath, sheet_name=\"threshold_raters\", header=None)\n",
    "    \n",
    "    if len(ref_data) != 0:\n",
    "        ref_data_cols = ref_data[0].tolist()\n",
    "    else:\n",
    "        ref_data_cols = []\n",
    "\n",
    "    print(\"Initialize data ingestion and file checking...\\n\")\n",
    "    \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], 'Deployment')\n",
    "        survey_path = ''\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], run_value, run_value_2)\n",
    "        survey_path = os.path.join(root_path, config[\"data_path\"][\"survey\"])\n",
    "       \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(data_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "    languages = []\n",
    "    for file in files:\n",
    "        for file_initial in file_initials:        \n",
    "            lang = file.split('_' + file_initial)[0]\n",
    "        if not lang.endswith((\".xlsx\")):\n",
    "            languages.append(lang)\n",
    "    \n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    file_groups = group_files_by_language(data_path, files, file_initials)\n",
    "    \n",
    "    file_exists = create_file_exists_df(files, file_initials)\n",
    "    \n",
    "    return data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - DEPLOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "def obtain_file_summary_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_summary = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Summary')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_summary.append(df)    \n",
    "        \n",
    "    return df_summary\n",
    "\n",
    "def obtain_file_data_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_data = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Data')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_data.append(df)    \n",
    "        \n",
    "    return df_data\n",
    "\n",
    "def obtain_distinct_raters(df_summary, ref_data_cols):\n",
    "\n",
    "    r1 = df_summary[0] # Joined data for Summary sheet from RC \n",
    "    r2 = df_summary[1] # Joined data for Summary page from Vocab_1 \n",
    "    r3 = df_summary[2] # Joined data for Summary page from Vocab_2 \n",
    "             \n",
    "    raters = pd.concat([r1,r2,r3], ignore_index=True)\n",
    "    raters = raters[['_worker_id', 'Grouping', 'Market', 'Language']]\n",
    "    raters = raters.drop_duplicates()\n",
    "    \n",
    "    if len(ref_data_cols) != 0:\n",
    "        \n",
    "        threshold_raters = ref_data_cols\n",
    "        raters = raters[raters['_worker_id'].isin(threshold_raters)]\n",
    "    \n",
    "    # obtain languages from r1 and create a dataframe\n",
    "    languages = r1.Language.unique().tolist()\n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    return raters, r1, r2, r3, languages\n",
    "\n",
    "def merge_raters_to_df_data(df_data, raters):\n",
    "\n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - PILOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_selection(root_path, config):\n",
    "\n",
    "    survey_path = os.path.join(root_path, config[\"data_path\"][\"survey\"])\n",
    "       \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(survey_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    survey_files = pd.DataFrame(files, columns = ['Survey Filename'])\n",
    "    \n",
    "    print(survey_files)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            survey_index = int(input(\"\\nPlease select the number of the survey filename for your pilot run: \"))\n",
    "            if survey_index < min(survey_files.index) or survey_index > max(survey_files.index):\n",
    "                print(f\"\\nYou must enter numbers between {min(survey_files.index)} - {max(survey_files.index)}... Please try again\")\n",
    "                continue\n",
    "            elif survey_index == \"\":\n",
    "                print(\"\\nYou must enter any numbers\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"\\nYou have selected {survey_index} for '{survey_files.iloc[survey_index, 0]}'\\n\")\n",
    "                survey_selected = survey_files.iloc[survey_index, 0]\n",
    "                break\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"\\nYou must enter numerical values only... Please try again\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return survey_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_survey_fluency(survey_data):\n",
    "    \n",
    "    fluency = []\n",
    "    for opt in survey_data['31_language_1']:\n",
    "        if opt == 'over_15_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '1015_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '510_years' :\n",
    "            fluency.append('Intermediate')\n",
    "        elif opt == '03_years' :\n",
    "            fluency.append('Not Fluent')   \n",
    "        else:\n",
    "            fluency.append('') \n",
    "            \n",
    "    return fluency\n",
    "\n",
    "def obtain_survey_data(survey_path, survey_selected):\n",
    "         \n",
    "    survey_data = pd.read_excel(os.path.join(survey_path, survey_selected), 'Sheet1')\n",
    "    try:\n",
    "        survey_data = survey_data.drop('Unnamed: 42', axis = 1)\n",
    "        survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "        survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "        survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "        survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "    except:\n",
    "        survey_data = survey_data\n",
    "        survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "        survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "        survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "        survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "            \n",
    "    return survey_data\n",
    "\n",
    "def merge_to_survey_data(df_data, raters, survey_data):\n",
    "    \n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Drop duplicat cols\n",
    "    rc = rc.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v1 = v1.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v2 = v2.drop(['Language_y', 'Market'], axis = 1)\n",
    "    \n",
    "    rc = rc.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v1 = v1.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v2 = v2.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Fluency'].isna(), 'Fluent', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Fluency'].isna(), 'Fluent', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Fluency'].isna(), 'Fluent', v2['Fluency'])\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    \n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize data ingestion and file checking...\n",
      "\n",
      "PASS: All files exists!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input the type of run e.g. Deployment, Pilot 1, Pilot 2, Pilot 3 .... etc.:  Pilot 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run type: Pilot 3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please input the pilot subfolder name e.g. Pilot 1A, Pilot 2C, Pilot 3A-B .... etc.:  Pilot 3A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pilot subfolder: Pilot 3A\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you know the 'Language' and/or 'Market code' for this file? (y/n) :  y\n",
      "\n",
      "Please enter the Language:  Chinese-Traditional\n",
      "\n",
      "Please enter the Market code: eg. EN-EN for English :  ZH-ZH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting automated data cleaning....\n",
      "\n",
      "Dataframe created from RC file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Missing columns inserted into 'Data' sheet.\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "              Language Market  _worker_id  Score  Percentage  Grouping\n",
      "0  Chinese-Traditional  ZH-ZH    45488857     19    0.791667  Pilot 3A\n",
      "1  Chinese-Traditional  ZH-ZH    45492033     21    0.875000  Pilot 3A\n",
      "2  Chinese-Traditional  ZH-ZH    45492267     21    0.875000  Pilot 3A\n",
      "3  Chinese-Traditional  ZH-ZH    45492778     20    0.833333  Pilot 3A\n",
      "4  Chinese-Traditional  ZH-ZH    45493055     19    0.791667  Pilot 3A\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "              Language           _id question_no_1 question_no_2  \\\n",
      "0  Chinese-Traditional  5.868399e+09             a             c   \n",
      "1  Chinese-Traditional  5.868408e+09             a             c   \n",
      "2  Chinese-Traditional  5.868455e+09             c             c   \n",
      "3  Chinese-Traditional  5.868504e+09             a             c   \n",
      "4  Chinese-Traditional  5.868596e+09             c             c   \n",
      "\n",
      "  question_no_3  question_no_4  question_no_5  \n",
      "0             a            NaN            NaN  \n",
      "1             a            NaN            NaN  \n",
      "2             a            NaN            NaN  \n",
      "3             a            NaN            NaN  \n",
      "4             a            NaN            NaN  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading RC raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : RC - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : RC - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : RC - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : RC - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : RC - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : RC - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-7 : RC - Data : checking if columns in the 'Data' sheet are identical to the reference columns ...\n",
      "\u001b[92mPASS\u001b[0m: The columns in the 'Data' sheet are identical to the reference\n",
      "\u001b[1m\n",
      "RC data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_1 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_1 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "              Language Market  _worker_id  Score  Percentage  Grouping\n",
      "0  Chinese-Traditional  ZH-ZH    45488857     28       0.700  Pilot 3A\n",
      "1  Chinese-Traditional  ZH-ZH    45492033     33       0.825  Pilot 3A\n",
      "2  Chinese-Traditional  ZH-ZH    45492267     30       0.750  Pilot 3A\n",
      "3  Chinese-Traditional  ZH-ZH    45492778     34       0.850  Pilot 3A\n",
      "4  Chinese-Traditional  ZH-ZH    45493055     35       0.875  Pilot 3A\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "              Language         _id rater_answer a_domain      a_register  \\\n",
      "0  Chinese-Traditional  5868341514          yes   people  slang/informal   \n",
      "1  Chinese-Traditional  5868371216          yes   people  slang/informal   \n",
      "2  Chinese-Traditional  5868388043           no   people  slang/informal   \n",
      "3  Chinese-Traditional  5868430242           no   people  slang/informal   \n",
      "4  Chinese-Traditional  5868447667          yes   people  slang/informal   \n",
      "\n",
      "  b_domain      b_register  \n",
      "0   people  slang/informal  \n",
      "1   people  slang/informal  \n",
      "2   people  slang/informal  \n",
      "3   people  slang/informal  \n",
      "4   people  slang/informal  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_1 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_1 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_1 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_1 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_1 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_1 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_1 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_1 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_2 file\n",
      "Language and Market columns and values inserted to 'Summary' sheet\n",
      "Language column and values inserted to 'Data' sheet\n",
      "Removing unwanted columns from Vocab_2 Data sheet\n",
      "\n",
      "Preview cleaned datasets:\n",
      "\n",
      "\n",
      "\n",
      "df_summary_cleaned\n",
      "\n",
      "\n",
      "              Language Market  _worker_id  Score  Percentage  Grouping\n",
      "0  Chinese-Traditional  ZH-ZH    45488857     73      0.9125  Pilot 3A\n",
      "1  Chinese-Traditional  ZH-ZH    45492033     68      0.8500  Pilot 3A\n",
      "2  Chinese-Traditional  ZH-ZH    45492267     71      0.8875  Pilot 3A\n",
      "3  Chinese-Traditional  ZH-ZH    45492778     72      0.9000  Pilot 3A\n",
      "4  Chinese-Traditional  ZH-ZH    45493055     64      0.8000  Pilot 3A\n",
      "\n",
      "\n",
      "df_data_cleaned\n",
      "\n",
      "\n",
      "              Language           _id               rater_answer   a_domain  \\\n",
      "0  Chinese-Traditional  5.868361e+09  a_is_more_specific_than_b  geography   \n",
      "1  Chinese-Traditional  5.868398e+09  a_is_more_specific_than_b  geography   \n",
      "2  Chinese-Traditional  5.868443e+09  a_is_more_specific_than_b  geography   \n",
      "3  Chinese-Traditional  5.868460e+09  a_is_more_specific_than_b  geography   \n",
      "4  Chinese-Traditional  5.868512e+09  a_is_more_specific_than_b  geography   \n",
      "\n",
      "  a_register   b_domain b_register  \n",
      "0    neutral  geography  technical  \n",
      "1    neutral  geography  technical  \n",
      "2    neutral  geography  technical  \n",
      "3    neutral  geography  technical  \n",
      "4    neutral  geography  technical  \n",
      "\n",
      "Data integrity report post clean-up:\n",
      "\n",
      "\u001b[1mReading Vocab_2 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_2 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[92mPASS\u001b[0m: 'Summary' sheet contains both 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-2 : Vocab_2 - Summary : Checking if Language' and 'Market' columns are empty ...\n",
      "\u001b[92mPASS\u001b[0m: Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
      "\n",
      "SCAN-3 : Vocab_2 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_2 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[92mPASS\u001b[0m: 'Data' sheet contains 'Language' columns\n",
      "\n",
      "SCAN-5 : Vocab_2 - Data : Checking if Language' column are empty ...\n",
      "\u001b[92mPASS\u001b[0m: 'Language'column in 'Data' contains complete data\n",
      "\n",
      "SCAN-6 : Vocab_2 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_2 data integrity result:\u001b[92m PASS\u001b[0m\n",
      "\n",
      "Automated data cleaning completed. Cleaned excel files are located in data > processed > Pilot 3 folder. \n",
      "\n",
      "Initialize data ingestion and file checking...\n",
      "\n",
      "               Survey Filename\n",
      "0  Survey Pilot 1A and 1B.xlsx\n",
      "1          Survey Pilot 2.xlsx\n",
      "2    Survey Pilot 2 and 3.xlsx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select the number of the survey filename for your pilot run:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have selected 2 for 'Survey Pilot 2 and 3.xlsx'\n",
      "\n",
      "              Language\n",
      "0  Chinese-Traditional\n",
      "\n",
      "Automated data processing completed.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "    \n",
    "    language, market, run_value, run_value_2  = data_cleaning.main()\n",
    "    \n",
    "    data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path = data_ingestion_initialize(root_path, run_value, run_value_2)\n",
    "    \n",
    "    df_summary = obtain_file_summary_df(file_initials, file_exists, data_path)\n",
    "    df_data = obtain_file_data_df(file_initials, file_exists, data_path)\n",
    "    raters, r1, r2, r3, languages =  obtain_distinct_raters(df_summary, ref_data_cols)\n",
    "        \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        rc, v1, v2 = merge_raters_to_df_data(df_data, raters)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        survey_selected = survey_selection(root_path, config)\n",
    "        survey_data = obtain_survey_data(survey_path, survey_selected)\n",
    "        rc, v1, v2 = merge_to_survey_data(df_data, raters, survey_data)\n",
    "    \n",
    "    return raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2 \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2  = main()\n",
    "    print(languages)\n",
    "    \n",
    "    print('\\nAutomated data processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "ala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
