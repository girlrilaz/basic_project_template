{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################       \n",
    "#Script Name    :                                                                                              \n",
    "#Description    :                                                                                 \n",
    "#Args           :                                                                                           \n",
    "#Author         : Nikhil Rao in R, converted to Python by Nor Raymond                                              \n",
    "#Email          : nraymond@appen.com                                          \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config_path = \"conf/base\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "except:\n",
    "    \n",
    "    os.chdir('..')\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_cleaning module\n",
    "import src.data.data_cleaning as data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to initialize data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_language(data_path, files, file_initials):\n",
    "    \n",
    "    file_groups = {}  \n",
    "    for x in files:  \n",
    "        key = x.split('_')[0] #x[:16] # The key is the first 16 characters of the file name\n",
    "        group = file_groups.get(key,[])\n",
    "        group.append(x)  \n",
    "        file_groups[key] = group\n",
    "                \n",
    "    return file_groups\n",
    "\n",
    "def create_file_exists_df(files, file_initials):\n",
    "    \n",
    "    checker = []\n",
    "    file_exists = []\n",
    "    for fname in files:\n",
    "        for key in file_initials:\n",
    "            if key in fname:\n",
    "                file_exists.append((key, fname))\n",
    "\n",
    "    file_exists = pd.DataFrame(file_exists, columns =['Keyword', 'Filename'])\n",
    "    \n",
    "    return file_exists\n",
    "\n",
    "def data_ingestion_initialize(root_path, run_value, run_value_2):\n",
    "    \n",
    "    # Function to load yaml configuration file\n",
    "    def load_config(config_name):\n",
    "        with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "    \n",
    "    # define reference file paths\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    ref_data = pd.read_excel(io = ref_filepath, sheet_name=\"threshold_raters\", header=None)\n",
    "    \n",
    "    if len(ref_data) != 0:\n",
    "        ref_data_cols = ref_data[0].tolist()\n",
    "    else:\n",
    "        ref_data_cols = []\n",
    "\n",
    "    print(\"Initialize data ingestion and file checking...\\n\")\n",
    "    \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], 'Deployment')\n",
    "        survey_path = ''\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], run_value, run_value_2)\n",
    "        survey_path = os.path.join(root_path, config[\"data_path\"][\"survey\"])\n",
    "       \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(data_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "    languages = []\n",
    "    for file in files:\n",
    "        for file_initial in file_initials:        \n",
    "            lang = file.split('_' + file_initial)[0]\n",
    "        if not lang.endswith((\".xlsx\")):\n",
    "            languages.append(lang)\n",
    "    \n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    file_groups = group_files_by_language(data_path, files, file_initials)\n",
    "    \n",
    "    file_exists = create_file_exists_df(files, file_initials)\n",
    "    \n",
    "    return data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - DEPLOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "def obtain_file_summary_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_summary = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Summary')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_summary.append(df)    \n",
    "        \n",
    "    return df_summary\n",
    "\n",
    "def obtain_file_data_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_data = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Data')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_data.append(df)    \n",
    "        \n",
    "    return df_data\n",
    "\n",
    "def obtain_distinct_raters(df_summary, ref_data_cols):\n",
    "\n",
    "    r1 = df_summary[0] # Joined data for Summary sheet from RC \n",
    "    r2 = df_summary[1] # Joined data for Summary page from Vocab_1 \n",
    "    r3 = df_summary[2] # Joined data for Summary page from Vocab_2 \n",
    "             \n",
    "    raters = pd.concat([r1,r2,r3], ignore_index=True)\n",
    "    raters = raters[['_worker_id', 'Grouping', 'Market', 'Language']]\n",
    "    raters = raters.drop_duplicates()\n",
    "    \n",
    "    if len(ref_data_cols) != 0:\n",
    "        \n",
    "        threshold_raters = ref_data_cols\n",
    "        raters = raters[raters['_worker_id'].isin(threshold_raters)]\n",
    "    \n",
    "    # obtain languages from r1 and create a dataframe\n",
    "    languages = r1.Language.unique().tolist()\n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    return raters, r1, r2, r3, languages\n",
    "\n",
    "def merge_raters_to_df_data(df_data, raters):\n",
    "\n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - PILOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_survey_fluency(survey_data):\n",
    "    \n",
    "    fluency = []\n",
    "    for opt in survey_data['31_language_1']:\n",
    "        if opt == 'over_15_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '1015_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '510_years' :\n",
    "            fluency.append('Intermediate')\n",
    "        elif opt == '03_years' :\n",
    "            fluency.append('Not Fluent')   \n",
    "        else:\n",
    "            fluency.append('') \n",
    "            \n",
    "    return fluency\n",
    "\n",
    "def obtain_survey_data(survey_path):\n",
    "    \n",
    "    # get the list of files in folder\n",
    "    files = os.listdir(survey_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    if len(files) == 1: \n",
    "        \n",
    "        survey_data = pd.read_excel(os.path.join(survey_path, files[0]), 'Sheet1')\n",
    "        try:\n",
    "            survey_data = survey_data.drop('Unnamed: 42', axis = 1)\n",
    "            survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "            survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "            survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "            survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "        except:\n",
    "            survey_data = survey_data\n",
    "            survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "            survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "            survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "            survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "            \n",
    "    if len(files) > 1:  \n",
    "        \n",
    "        print('FAIL: reference > Survey folder contains more than 1 file! It should only be the relevant survey file for the pilot assessment.')\n",
    "        survey_data = ''\n",
    "    \n",
    "    if len(files) ==0:  \n",
    "        \n",
    "        print('FAIL: reference > Survey folder contains no file! It should contain one relevant survey file for the pilot assessment.')\n",
    "        survey_data = ''\n",
    "    \n",
    "    return survey_data\n",
    "\n",
    "def merge_to_survey_data(df_data, raters, survey_data):\n",
    "    \n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Drop duplicat cols\n",
    "    rc = rc.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v1 = v1.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v2 = v2.drop(['Language_y', 'Market'], axis = 1)\n",
    "    \n",
    "    rc = rc.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v1 = v1.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v2 = v2.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Fluency'].isna(), 'Fluent', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Fluency'].isna(), 'Fluent', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Fluency'].isna(), 'Fluent', v2['Fluency'])\n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "# survey_path = '/Users/nraymond/Documents/Work/Account/Google/Arrow/ALA_automation/data/reference/Survey'\n",
    "# survey_data = obtain_survey_data(survey_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "    \n",
    "    language, market, run_value, run_value_2  = data_cleaning.main()\n",
    "    \n",
    "    data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path = data_ingestion_initialize(root_path, run_value, run_value_2)\n",
    "    \n",
    "    df_summary = obtain_file_summary_df(file_initials, file_exists, data_path)\n",
    "    df_data = obtain_file_data_df(file_initials, file_exists, data_path)\n",
    "    raters, r1, r2, r3, languages =  obtain_distinct_raters(df_summary, ref_data_cols)\n",
    "        \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        rc, v1, v2 = merge_raters_to_df_data(df_data, raters)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        survey_data = obtain_survey_data(survey_path)\n",
    "        rc, v1, v2 = merge_to_survey_data(df_data, raters, survey_data)\n",
    "    \n",
    "    return raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2 \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2  = main()\n",
    "    print(languages)\n",
    "    \n",
    "    print('\\nAutomated data processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "ala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
