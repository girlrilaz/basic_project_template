{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################       \n",
    "#Script Name    :                                                                                              \n",
    "#Description    :                                                                                 \n",
    "#Args           :                                                                                           \n",
    "#Author         : Nikhil Rao in R, converted to Python by Nor Raymond                                              \n",
    "#Email          : nraymond@appen.com                                          \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config_path = \"conf/base\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "except:\n",
    "    \n",
    "    os.chdir('..')\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_cleaning module\n",
    "import src.data.data_cleaning as data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to initialize data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_language(data_path, files, file_initials):\n",
    "    \n",
    "    file_groups = {}  \n",
    "    for x in files:  \n",
    "        key = x.split('_')[0] #x[:16] # The key is the first 16 characters of the file name\n",
    "        group = file_groups.get(key,[])\n",
    "        group.append(x)  \n",
    "        file_groups[key] = group\n",
    "                \n",
    "    return file_groups\n",
    "\n",
    "def create_file_exists_df(files, file_initials):\n",
    "    \n",
    "    checker = []\n",
    "    file_exists = []\n",
    "    for fname in files:\n",
    "        for key in file_initials:\n",
    "            if key in fname:\n",
    "                file_exists.append((key, fname))\n",
    "\n",
    "    file_exists = pd.DataFrame(file_exists, columns =['Keyword', 'Filename'])\n",
    "    \n",
    "    return file_exists\n",
    "\n",
    "def data_ingestion_initialize(root_path, run_value, run_value_2):\n",
    "    \n",
    "    # Function to load yaml configuration file\n",
    "    def load_config(config_name):\n",
    "        with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "    \n",
    "    # define reference file paths\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    ref_data = pd.read_excel(io = ref_filepath, sheet_name=\"threshold_raters\", header=None)\n",
    "    \n",
    "    if len(ref_data) != 0:\n",
    "        ref_data_cols = ref_data[0].tolist()\n",
    "    else:\n",
    "        ref_data_cols = []\n",
    "\n",
    "    print(\"Initialize data ingestion and file checking...\\n\")\n",
    "    \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], 'Deployment')\n",
    "        survey_path = ''\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # define data input paths\n",
    "        data_path = os.path.join(root_path, config[\"data_path\"][\"output\"], run_value, run_value_2)\n",
    "        survey_path = os.path.join(root_path, config[\"data_path\"][\"survey\"])\n",
    "        post_survey_path = os.path.join(root_path, config[\"data_path\"][\"post_survey\"])\n",
    "        \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(data_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "    languages = []\n",
    "    for file in files:\n",
    "        for file_initial in file_initials:        \n",
    "            lang = file.split('_' + file_initial)[0]\n",
    "        if not lang.endswith((\".xlsx\")):\n",
    "            languages.append(lang)\n",
    "    \n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    file_groups = group_files_by_language(data_path, files, file_initials)\n",
    "    \n",
    "    file_exists = create_file_exists_df(files, file_initials)\n",
    "    \n",
    "    return data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path, post_survey_path\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - DEPLOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "def obtain_file_summary_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_summary = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Summary')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_summary.append(df)    \n",
    "        \n",
    "    return df_summary\n",
    "\n",
    "def obtain_file_data_df(file_initials, file_exists, data_path):\n",
    "    \n",
    "    df_data = []\n",
    "    for k in file_initials:\n",
    "        selected_files = file_exists[file_exists['Keyword'] == k] \n",
    "        selected_filenames = selected_files['Filename'].tolist()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for f in selected_filenames:\n",
    "            data = pd.read_excel(os.path.join(data_path, f), 'Data')\n",
    "            df = df.append(data)\n",
    "\n",
    "        df_data.append(df)  \n",
    "        \n",
    "    return df_data\n",
    "\n",
    "def obtain_distinct_raters(df_summary, ref_data_cols):\n",
    "\n",
    "    r1 = df_summary[0] # Joined data for Summary sheet from RC \n",
    "    r2 = df_summary[1] # Joined data for Summary page from Vocab_1 \n",
    "    r3 = df_summary[2] # Joined data for Summary page from Vocab_2 \n",
    "             \n",
    "    raters = pd.concat([r1,r2,r3], ignore_index=True)\n",
    "    raters = raters[['_worker_id', 'Grouping', 'Market', 'Language']]\n",
    "    raters = raters.drop_duplicates()\n",
    "    \n",
    "    if len(ref_data_cols) != 0:\n",
    "        \n",
    "        threshold_raters = ref_data_cols\n",
    "        raters = raters[raters['_worker_id'].isin(threshold_raters)]\n",
    "    \n",
    "    # obtain languages from r1 and create a dataframe\n",
    "    languages = r1.Language.unique().tolist()\n",
    "    languages = pd.DataFrame(languages, columns = ['Language'])\n",
    "    \n",
    "    return raters, r1, r2, r3, languages\n",
    "\n",
    "def merge_raters_to_df_data(df_data, raters):\n",
    "\n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', on=['_worker_id', 'Language'])\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing - PILOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pilot_selection(root_path, config):\n",
    "\n",
    "##  read pilot variation data from reference_check.xlsx file\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    pilot_variation = pd.read_excel(io = ref_filepath, sheet_name=\"pilot_variation\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(pilot_variation)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            pilot_index = int(input(\"\\nPlease select the number of the pilot variation: \"))\n",
    "            if pilot_index < min(pilot_variation.index) or pilot_index > max(pilot_variation.index):\n",
    "                print(f\"\\nYou must enter numbers between {min(pilot_variation.index)} - {max(pilot_variation.index)}... Please try again\")\n",
    "                continue\n",
    "            elif pilot_index == \"\":\n",
    "                print(\"\\nYou must enter any numbers\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"\\nYou have selected {pilot_index} for '{pilot_variation.iloc[pilot_index, 1]}'\\n\")\n",
    "                pilot_selected = pilot_variation.iloc[pilot_index, 0]\n",
    "                pilot_var_selected = pilot_variation.iloc[pilot_index, 1]\n",
    "                break\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"\\nYou must enter numerical values only... Please try again\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return pilot_variation, pilot_selected, pilot_var_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_selection(root_path, config):\n",
    "\n",
    "    survey_path = os.path.join(root_path, config[\"data_path\"][\"survey\"])\n",
    "       \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(survey_path)\n",
    "    files = [f for f in files if f[-5:] == '.xlsx']\n",
    "    \n",
    "    survey_files = pd.DataFrame(files, columns = ['Survey Filename'])\n",
    "    \n",
    "    print(survey_files)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            survey_index = int(input(\"\\nPlease select the number of the survey filename for your pilot run: \"))\n",
    "            if survey_index < min(survey_files.index) or survey_index > max(survey_files.index):\n",
    "                print(f\"\\nYou must enter numbers between {min(survey_files.index)} - {max(survey_files.index)}... Please try again\")\n",
    "                continue\n",
    "            elif survey_index == \"\":\n",
    "                print(\"\\nYou must enter any numbers\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"\\nYou have selected {survey_index} for '{survey_files.iloc[survey_index, 0]}'\\n\")\n",
    "                survey_selected = survey_files.iloc[survey_index, 0]\n",
    "                break\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"\\nYou must enter numerical values only... Please try again\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return survey_selected, survey_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for common pilot operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_survey_fluency(survey_data):\n",
    "    \n",
    "    fluency = []\n",
    "    for opt in survey_data['31_language_1']:\n",
    "        if opt == 'over_15_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '1015_years' :\n",
    "            fluency.append('Fluent')\n",
    "        elif opt == '510_years' :\n",
    "            fluency.append('Intermediate')\n",
    "        elif opt == '03_years' :\n",
    "            fluency.append('Not Fluent')   \n",
    "        else:\n",
    "            fluency.append('') \n",
    "            \n",
    "    return fluency\n",
    "\n",
    "def obtain_survey_data(survey_path, survey_selected):\n",
    "         \n",
    "    survey_data = pd.read_excel(os.path.join(survey_path, survey_selected), 'Sheet1')\n",
    "    try:\n",
    "        survey_data = survey_data.drop('Unnamed: 42', axis = 1)\n",
    "        survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "        survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "        survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "        survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "    except:\n",
    "        survey_data = survey_data\n",
    "        survey_data[['_created_at','_started_at']] = survey_data[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "        survey_data = survey_data.rename(columns = {\"_created_at\" : \"survey_created_at\", \"_started_at\" : \"survey_started_at\"})\n",
    "        survey_data = survey_data[['_worker_id', '31_language_1', 'survey_created_at', 'survey_started_at']]\n",
    "        survey_data['Fluency'] = obtain_survey_fluency(survey_data)\n",
    "            \n",
    "    return survey_data\n",
    "\n",
    "def merge_to_survey_data(df_data, raters, survey_data):\n",
    "    \n",
    "    rc = df_data[0] # Joined data for Data sheet from RC \n",
    "    v1 = df_data[1] # Joined data for Data page from Vocab_1 \n",
    "    v2 = df_data[2] # Joined data for Data page from Vocab_2 \n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, raters,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Merge raters data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, survey_data,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    # Drop duplicate cols\n",
    "    rc = rc.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v1 = v1.drop(['Language_y', 'Market'], axis = 1)\n",
    "    v2 = v2.drop(['Language_y', 'Market'], axis = 1)\n",
    "    \n",
    "    rc = rc.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v1 = v1.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    v2 = v2.rename(columns = {\"Language_x\":\"Language\"})\n",
    "    \n",
    "    # Convert _created_at and _started_at to date-time\n",
    "    rc[['_created_at','_started_at']] = rc[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v1[['_created_at','_started_at']] = v1[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    v2[['_created_at','_started_at']] = v2[['_created_at','_started_at']].apply(pd.to_datetime, format='%m/%d/%Y %H:%M:%S')\n",
    "    \n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for varied pilot operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and merge to post survey data\n",
    "# Note: functions relevant to Pilot 1A-1B\n",
    "def obtain_post_survey_data(post_survey_path, pilot_selected, pilot_var_selected):\n",
    "\n",
    "    '''\n",
    "    Valid for Pilot 1A-1B\n",
    "    '''\n",
    "        \n",
    "    post_survey_path = os.path.join(root_path, post_survey_path)\n",
    "    post_survey_data = pd.read_excel(os.path.join(post_survey_path, pilot_selected, pilot_var_selected, 'Post Assessment Survey.xlsx'), 'Sheet1')\n",
    "    \n",
    "    postsurvey = post_survey_data[['_worker_id', 'question_1', 'question_1_1', 'question_2', 'question_2_1', 'question_3', 'question_3_1',\n",
    "                                   'question_4', 'question_4_1', 'question_5', 'question_5_1', 'question_6', 'question_6_1']]\n",
    "    \n",
    "    postsurvey.columns = [\"_worker_id\",\n",
    "                          \"Did you feel any questions or content was inappropriate or offensive?\",\n",
    "                          \"If you answered yes, please identify which question? Please explain\",\n",
    "                          \"Did you notice any issues with the language? i.e., grammar, spelling, archaic terminology?\",\n",
    "                          \"If so, what were the issues?\",\n",
    "                          \"Did you feel you had enough time to complete the assessment in one sitting?\",\n",
    "                          \"If so, did you feel pressured for time? Please share your thoughts and experience.\",\n",
    "                          \"Did you feel there was a good range of difficulty in the questions?\",\n",
    "                          \"Please explain your thoughts? Any details you can offer are greatly appreciated.\",\n",
    "                          \"Did you use Google search / Google Translate / Word Reference / Linguee / any other sources to help you complete the test?\",\n",
    "                          \"If so, please list what you used, and explain why you chose to use them?\",\n",
    "                          \"If you did not finish the full assessment, we would love to know why. Did you find it too time consuming? Did you get interrupted?\",\n",
    "                          \"If so, any information you can provide is helpful.\"]\n",
    "    \n",
    "    return postsurvey\n",
    "\n",
    "def obtain_tenure_grouping(dfs, pilot_var_selected):\n",
    "    \n",
    "    '''\n",
    "    Valid for Pilot 1A-1B if Grouping == GT -> Google Translate Control Group\n",
    "    Valid for Pilot 1C - Grouping == GT -> GT\n",
    "    '''\n",
    "       \n",
    "    tenure = []\n",
    "    for opt in dfs['Grouping']:\n",
    "        if opt == 'Pilot 1A' :\n",
    "            tenure.append('90+ days')\n",
    "        elif opt == 'Pilot 1B' :\n",
    "            tenure.append('1 - 90 days')\n",
    "        elif opt == 'GT' :\n",
    "            if pilot_var_selected == 'Pilot 1A-1B':\n",
    "                tenure.append('Google Translate Control Group')\n",
    "            elif pilot_var_selected == 'Pilot 1C':\n",
    "                tenure.append('GT')\n",
    "        else:\n",
    "            tenure.append('') \n",
    "            \n",
    "    return tenure\n",
    "\n",
    "def merge_to_post_survey_data(rc, v1, v2, postsurvey, pilot_var_selected):\n",
    "    \n",
    "    '''\n",
    "    Valid for Pilot 1A-1B\n",
    "    '''\n",
    "    \n",
    "    # Merge postsurvey data to v1, v2, and rc\n",
    "    rc = pd.merge(rc, postsurvey,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v1 = pd.merge(v1, postsurvey,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    v2 = pd.merge(v2, postsurvey,  how='left', left_on=['_worker_id'], right_on=['_worker_id'])\n",
    "    \n",
    "    rc, v1, v2 = fluency_categorization_group_3(rc, v1, v2)\n",
    "    \n",
    "        \n",
    "    rc['Tenure'] = obtain_tenure_grouping(rc, pilot_var_selected)\n",
    "    v1['Tenure'] = obtain_tenure_grouping(v1, pilot_var_selected)    \n",
    "    v2['Tenure'] = obtain_tenure_grouping(v2, pilot_var_selected)    \n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "\n",
    "# Fluency categorization functions\n",
    "# Note: missing group 3A-A , 2A-A has to be manual\n",
    "\n",
    "def fluency_categorization_group_1(rc, v1, v2):\n",
    "    \n",
    "    '''\n",
    "    Fail Rates : Valid for 1C , 1D , 2A\n",
    "    Grand Summary :  1A-1B\n",
    "    '''\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    rc = rc.dropna(subset=['Fluency'])\n",
    "    v1 = v1.dropna(subset=['Fluency'])\n",
    "    v2 = v2.dropna(subset=['Fluency'])\n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "def fluency_categorization_group_2(rc, v1, v2):\n",
    "    \n",
    "    '''\n",
    "    Fail Rates : Valid for 1E, 1E(ES) , 2D , 2B-A , 3A\n",
    "    Grand Summary : Valid for 1E, 1E(ES) , 2D , 2B-A , 3A\n",
    "    '''\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Fluency'].isna(), 'Fluent', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Fluency'].isna(), 'Fluent', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Fluency'].isna(), 'Fluent', v2['Fluency'])\n",
    "    \n",
    "    return rc, v1, v2\n",
    "\n",
    "def fluency_categorization_group_3(rc, v1, v2):\n",
    "    \n",
    "    '''\n",
    "    Fail Rates : Valid for 1A-1B\n",
    "    '''\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['31_language_1'].isna(), 'Not Categorized', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['31_language_1'].isna(), 'Not Categorized', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['31_language_1'].isna(), 'Not Categorized', v2['Fluency'])\n",
    "    \n",
    "    rc['Fluency'] = np.where(rc['Grouping'] == 'GT', 'GT', rc['Fluency'])\n",
    "    v1['Fluency'] = np.where(v1['Grouping'] == 'GT', 'GT', v1['Fluency'])\n",
    "    v2['Fluency'] = np.where(v2['Grouping'] == 'GT', 'GT', v2['Fluency'])\n",
    "    \n",
    "    return rc, v1, v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "    \n",
    "    language, market, run_value, run_value_2  = data_cleaning.main()\n",
    "    \n",
    "    data_path, files, languages, file_groups, file_exists, ref_data_cols, survey_path, post_survey_path = data_ingestion_initialize(root_path, run_value, run_value_2)\n",
    "    \n",
    "    df_summary = obtain_file_summary_df(file_initials, file_exists, data_path)\n",
    "    df_data = obtain_file_data_df(file_initials, file_exists, data_path)\n",
    "    raters, r1, r2, r3, languages =  obtain_distinct_raters(df_summary, ref_data_cols)\n",
    "        \n",
    "    if run_value == 'Deployment':\n",
    "        \n",
    "        rc, v1, v2 = merge_raters_to_df_data(df_data, raters)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        pilot_variation, pilot_selected, pilot_var_selected = pilot_selection(root_path, config)\n",
    "        survey_selected, survey_files = survey_selection(root_path, config)\n",
    "        survey_data = obtain_survey_data(survey_path, survey_selected)\n",
    "        rc, v1, v2 = merge_to_survey_data(df_data, raters, survey_data)\n",
    "        \n",
    "        if pilot_var_selected == 'Pilot 1A-1B':\n",
    "            \n",
    "            # Create Grand Summary first\n",
    "            rcS, v1S, v2S = fluency_categorization_group_1(rc, v1, v2)\n",
    "            \n",
    "            # Data processing for Fail Rates report\n",
    "            postsurvey = obtain_post_survey_data(post_survey_path, pilot_selected, pilot_var_selected)\n",
    "            rc, v1, v2 = merge_to_post_survey_data(rc, v1, v2, postsurvey, pilot_var_selected)\n",
    "            \n",
    "            \n",
    "        elif pilot_var_selected == 'Pilot 1C':\n",
    "            \n",
    "            # Create Grand Summary first\n",
    "            rcS, v1S, v2S = fluency_categorization_group_1(rc, v1, v2)\n",
    "            \n",
    "            # Data processing for Fail Rates report\n",
    "            rc, v1, v2 = fluency_categorization_group_1(rc, v1, v2)\n",
    "            rc['Tenure'] = obtain_tenure_grouping(rc, pilot_var_selected)\n",
    "            v1['Tenure'] = obtain_tenure_grouping(v1, pilot_var_selected)    \n",
    "            v2['Tenure'] = obtain_tenure_grouping(v2, pilot_var_selected) \n",
    "            \n",
    "            \n",
    "        elif pilot_var_selected == 'Pilot 1D' or pilot_var_selected == 'Pilot 2A':\n",
    "            \n",
    "            # Create Grand Summary first\n",
    "            rcS, v1S, v2S = fluency_categorization_group_1(rc, v1, v2)\n",
    "            \n",
    "             # Data processing for Fail Rates report\n",
    "            rc, v1, v2 = fluency_categorization_group_1(rc, v1, v2)   \n",
    "            \n",
    "        elif (pilot_var_selected == 'Pilot 1E' or pilot_var_selected == 'Pilot 1E(ES)' or \n",
    "              pilot_var_selected == 'Pilot 2D' or pilot_var_selected == 'Pilot 2B-A' or\n",
    "              pilot_var_selected == 'Pilot 3A'):\n",
    "            \n",
    "            # Create Grand Summary first\n",
    "            rcS, v1S, v2S = fluency_categorization_group_2(rc, v1, v2) \n",
    "            \n",
    "            # Data processing for Fail Rates report\n",
    "            rc, v1, v2 = fluency_categorization_group_2(rc, v1, v2)   \n",
    "            \n",
    " \n",
    "    return raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2, survey_selected, survey_files, pilot_variation, pilot_selected, pilot_var_selected, rcS, v1S, v2S  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    raters, r1, r2, r3, languages, rc, v1, v2, run_value, run_value_2, survey_selected, survey_files, pilot_variation, pilot_selected, pilot_var_selected, rcS, v1S, v2S  = main()\n",
    "    print(languages)\n",
    "    \n",
    "    print('\\nAutomated data processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "ala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
