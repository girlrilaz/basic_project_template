{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################       \n",
    "#Script Name    :                                                                                              \n",
    "#Description    :                                                                                 \n",
    "#Args           :                                                                                           \n",
    "#Author         : Nor Raymond                                                \n",
    "#Email          : nraymond@appen.com                                          \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(config_path, config_name), 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config_path = \"conf/base\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "except:\n",
    "    \n",
    "    os.chdir('..')\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    os.chdir(config[\"project_path\"])\n",
    "    root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to initialize data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize data ingestion and file checking...\n",
      "PASS: All files exists!\n"
     ]
    }
   ],
   "source": [
    "def raw_file_checker(files): \n",
    "\n",
    "    keyword = ['RC', 'Vocab_2', 'Vocab_1']\n",
    "    checker = []\n",
    "    file_exists = {}\n",
    "    for fname in files:\n",
    "        for key in keyword:\n",
    "            if key in fname:\n",
    "                checker.append(True)\n",
    "                file_exists[key] = os.path.join(fname)\n",
    "                \n",
    "    if len(checker) == 3 :\n",
    "        print(\"PASS: All files exists!\")\n",
    "        condition = True\n",
    "    else:\n",
    "        print(\"FAIL: Not all file exists! Please check the raw data folder to ensure RC, Vocab_1 and Vocab_2 file exists.\")\n",
    "        condition = False\n",
    "        \n",
    "    return condition, file_exists\n",
    "\n",
    "\n",
    "def data_ingestion_initialize(root_path):\n",
    "    \n",
    "    # Function to load yaml configuration file\n",
    "    def load_config(config_name):\n",
    "        with open(os.path.join(root_path, config_path, config_name), 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        return config\n",
    "\n",
    "    # load yaml catalog configuration file\n",
    "    config = load_config(\"catalog.yml\")\n",
    "\n",
    "    print(\"Initialize data ingestion and file checking...\")\n",
    "    \n",
    "    # define input and output data paths\n",
    "    raw_data_path = os.path.join(root_path, config[\"data_path\"][\"input\"])\n",
    "    out_data_path = os.path.join(root_path, config[\"data_path\"][\"output\"])\n",
    "    \n",
    "    # define reference file paths\n",
    "    ref_path = os.path.join(root_path, config[\"data_path\"][\"ref\"])\n",
    "    ref_filepath = os.path.join(ref_path, config[\"filenames\"][\"rc_col_ref\"])\n",
    "    ref_data = pd.read_excel(io = ref_filepath, sheet_name=\"columns_check\", header=None)\n",
    "    ref_data_cols = ref_data[0].tolist()\n",
    "    \n",
    "    # get the list of files in raw folder\n",
    "    files = os.listdir(raw_data_path)\n",
    "    files = [f for f in files if f[-4:] == '.xls']\n",
    "    \n",
    "    condition, file_exists = raw_file_checker(files)\n",
    "    \n",
    "    ## Define raw data filepaths\n",
    "    rc_filepath = os.path.join(raw_data_path, file_exists['RC'])\n",
    "    v1_filepath = os.path.join(raw_data_path, file_exists['Vocab_1'])\n",
    "    v2_filepath = os.path.join(raw_data_path, file_exists['Vocab_2'])\n",
    "       \n",
    "    return raw_data_path, out_data_path, ref_path, ref_filepath, ref_data, ref_data_cols, files, file_exists, rc_filepath, v1_filepath, v2_filepath\n",
    "\n",
    "raw_data_path, out_data_path, ref_path, ref_filepath, ref_data, ref_data_cols, files, file_exists, rc_filepath, v1_filepath, v2_filepath = data_ingestion_initialize(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(file_initial, rc_filepath, v1_filepath , v2_filepath):\n",
    "    \n",
    "    '''\n",
    "    file_initial choices -\n",
    "    RC: Reading Comprehension \n",
    "    Vocab_1: Vocabulary 1 \n",
    "    Vocab_2: Vocabulary 2\n",
    "    '''\n",
    "    \n",
    "    if file_initial == 'RC':\n",
    "        filepath = rc_filepath\n",
    "    elif file_initial == 'Vocab_1':\n",
    "        filepath = v1_filepath\n",
    "    elif file_initial == 'Vocab_2':\n",
    "        filepath = v2_filepath\n",
    "    \n",
    "    # create dataframe from 'Summary' sheet\n",
    "    df_summary = pd.read_excel(io = filepath, sheet_name=\"Summary\")\n",
    "    df_summary_cols = list(df_summary.columns)\n",
    "    \n",
    "    # create dataframe from 'Data' sheet\n",
    "    df_data = pd.read_excel(io=filepath, sheet_name=\"Data\")\n",
    "    df_data_cols = list(df_data.columns)\n",
    "    \n",
    "    # create dataframe from 'Data' sheet\n",
    "    df_ans_key = pd.read_excel(io=filepath, sheet_name=\"Answer Key\")\n",
    "    df_ans_key_cols = list(df_ans_key.columns)\n",
    "    \n",
    "    print(f\"Dataframe created from {file_initial} file\")\n",
    "    \n",
    "    return df_summary, df_summary_cols, df_data, df_data_cols, df_ans_key, df_ans_key_cols\n",
    "\n",
    "#df_summary, df_summary_cols, df_data, df_data_cols, df_ans_key, df_ans_key_cols = create_dataframes('RC', rc_filepath, v1_filepath , v2_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integrity scanning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "def print_scan_results(col_condition_num, scan_num, file_initial , sheets = 'Summary'):\n",
    "    \n",
    "    if scan_num == 1:\n",
    "        print(f\"\\nSCAN-{scan_num} : {file_initial} - {sheets} : Checking if the sheet contains either 'Language' and 'Market' columns ...\")\n",
    "        if col_condition_num == True:\n",
    "            print(color.GREEN + \"PASS\" + color.END + \": 'Summary' sheet contains both 'Language' and 'Market' columns\")\n",
    "        else: \n",
    "            print(color.RED + \"FAIL\" + color.END + \": 'Summary' sheet does not contain either 'Language' and 'Market' columns\")\n",
    "\n",
    "    if scan_num == 2:\n",
    "        print(f\"\\nSCAN-{scan_num} : {file_initial} - {sheets} : Checking if Language' and 'Market' columns are empty ...\")\n",
    "        if col_condition_num == True:\n",
    "            print(color.GREEN + \"PASS\" + color.END + \": Both 'Language' and 'Market' columns in 'Summary' contains complete data\")\n",
    "        else: \n",
    "            print(color.RED + \"FAIL\" + color.END + \": Both or either 'Language' and 'Market' columns in 'Summary' sheet are empty or incomplete\")\n",
    "            \n",
    "    if scan_num == 3 or scan_num == 6:\n",
    "        print(f\"\\nSCAN-{scan_num} : {file_initial} - {sheets} : Checking if '_worker_id' column name is correct ...\")\n",
    "        if col_condition_num == True:\n",
    "            print(color.GREEN + \"PASS\" + color.END + \": valid '_workder_id' column name\")\n",
    "        else:\n",
    "            print(color.RED + \"FAIL\" + color.END + \": invalid '_workder_id' column name\")\n",
    "            \n",
    "    if scan_num == 4:         \n",
    "        print(f\"\\nSCAN-{scan_num} : {file_initial} - {sheets} : Checking if sheet contains 'Language' column ...\")\n",
    "        if col_condition_num == True:\n",
    "            print(color.GREEN + \"PASS\" + color.END + \": 'Data' sheet contains 'Language' columns\")\n",
    "        else: \n",
    "            print(color.RED + \"FAIL\" + color.END + \": 'Data' sheet does not contain 'Language' columns\")\n",
    "            \n",
    "    if scan_num == 5:         \n",
    "        print(f\"\\nSCAN-{scan_num} : {file_initial} - {sheets} : Checking if Language' column are empty ...\")\n",
    "        if col_condition_num == True:\n",
    "            print(color.GREEN + \"PASS\" + color.END + \": 'Language'column in 'Data' contains complete data\")\n",
    "        else: \n",
    "            print(color.RED + \"FAIL\" + color.END + \": 'Language' column in 'Data' sheet are empty or incomplete\")\n",
    "            \n",
    "    if scan_num == 7 and file_initial == 'RC':         \n",
    "        print(f\"\\nSCAN-7 : {file_initial} - {sheets} : checking if columns in the 'Data' sheet are identical to the reference columns ...\")\n",
    "        if col_condition_num == True:\n",
    "            print (color.GREEN + \"PASS\" + color.END + \": The columns in the 'Data' sheet are identical to the reference\") \n",
    "        else: \n",
    "            print (color.RED + \"FAIL\" + color.END + \": The columns in the 'Data' sheet are not identical to the reference\")         \n",
    "            \n",
    "def summary_col_check(df_summary, df_summary_cols, file_initial , sheets = 'Summary'): \n",
    "      \n",
    "    # --- SCAN-1 : checking if \"Summary\" sheet contains \"Language\" and \"Market\" columns   ---------------------\n",
    "    # PASS -> 'Summary' sheet contains both 'Language' and 'Market' columns\n",
    "    scan_num = 1\n",
    "    cols_to_check = ['Language', 'Market']\n",
    "    col_checker = {}   \n",
    "    for col in cols_to_check:\n",
    "        \n",
    "        if col in df_summary_cols:\n",
    "            col_checker[col] = True\n",
    "        else:\n",
    "            col_checker[col] = False\n",
    "            \n",
    "    condition_1 = col_checker['Language']\n",
    "    condition_2 = col_checker['Market']\n",
    "    col_condition_1 = all([condition_1, condition_2]) # both conditions has to be true\n",
    "\n",
    "    return col_condition_1, scan_num\n",
    "\n",
    "def summary_col_value_check(df_summary, file_initial, sheets = 'Summary'): \n",
    "    \n",
    "    # --- SCAN-2 :checking if \"Language\" and \"Market\" columns in \"Summary\" is empty   -------------------------\n",
    "    # PASS -> Both 'Language' and 'Market' columns in 'Summary' contains complete data\n",
    "    scan_num = 2\n",
    "    cols_to_check = ['Language', 'Market']\n",
    "    col_checker = {}\n",
    "    for col in cols_to_check:\n",
    "\n",
    "        if df_summary[col].notnull().values.all() == True:\n",
    "            col_checker[col] = True\n",
    "        else:\n",
    "            col_checker[col] = False\n",
    "\n",
    "    condition_3 = col_checker['Language']\n",
    "    condition_4 = col_checker['Market']\n",
    "    col_condition_2 = all([condition_3, condition_4]) # both conditions has to be true\n",
    "\n",
    "    return col_condition_2, scan_num\n",
    "\n",
    "def col_header_check(df_summary_data, file_initial, sheets):\n",
    "    \n",
    "    # --- SCAN-3 : checking if worker_id column contains _ at the start   -------------------------------------\n",
    "    # PASS -> if the number of character is 10 not 9 and column name is _workder_id\n",
    "    scan_num = 3\n",
    "    find_worker_idx = df_summary_data.columns.str.contains('worker')\n",
    "    worker_idx = [i for i, x in enumerate(find_worker_idx) if x][0]\n",
    "    worker_col = df_summary_data.columns[worker_idx]\n",
    "    worker_col_len = len(worker_col)\n",
    "    \n",
    "    if worker_col_len == 10 and worker_col[0] == \"_\":\n",
    "        col_condition_3 = True\n",
    "    elif worker_col_len == 9 and worker_col[0] == \"w\":\n",
    "        col_condition_3 = False\n",
    "    return col_condition_3, scan_num\n",
    "\n",
    "def data_col_check(df_data, df_data_cols, file_initial, sheets = 'Data'): \n",
    "      \n",
    "    # --- SCAN-4 : checking if \"Data\" sheet contains \"Language\" column   --------------------------------------\n",
    "    # PASS -> 'Data' sheet contains both 'Language' column\n",
    "    scan_num = 4\n",
    "    cols_to_check = ['Language']\n",
    "    col_checker = {}   \n",
    "    for col in cols_to_check:\n",
    "        \n",
    "        if col in df_data_cols:\n",
    "            col_checker[col] = True\n",
    "        else:\n",
    "            col_checker[col] = False\n",
    "            \n",
    "    condition_1 = col_checker['Language']\n",
    "    col_condition_4 = all([condition_1])\n",
    "        \n",
    "    return col_condition_4, scan_num\n",
    "\n",
    "def data_col_value_check(df_data, file_initial, sheets = 'Data'): \n",
    "    \n",
    "    # --- SCAN-5 :checking if \"Language\" column in \"Data\" is empty   -------------------------\n",
    "    # PASS -> 'Language' column in 'Data' contains complete data\n",
    "    scan_num = 5\n",
    "    cols_to_check = ['Language']\n",
    "    col_checker = {}\n",
    "    for col in cols_to_check:\n",
    "\n",
    "        if df_data[col].notnull().values.all() == True:\n",
    "            col_checker[col] = True\n",
    "        else:\n",
    "            col_checker[col] = False\n",
    "\n",
    "    condition_3 = col_checker['Language']\n",
    "    col_condition_5 = all([condition_3])\n",
    "\n",
    "    return col_condition_5, scan_num\n",
    "\n",
    "def data_col_header_check(df_data_cols, ref_data_cols, file_initial, sheets = 'Data'):\n",
    "    \n",
    "    # --- SCAN-7 : checking if columns in \"Data\" sheet are identical to the reference columns   ------------------------\n",
    "    # refer to the file in reference > reference_checks.xlsx\n",
    "    # PASS -> if the two column lists are identical\n",
    "    scan_num = 7\n",
    "    ref_data_cols_sorted = ref_data_cols\n",
    "    df_data_cols_sorted = df_data_cols\n",
    "    \n",
    "    # sorting both the lists \n",
    "    ref_data_cols_sorted.sort() \n",
    "    df_data_cols_sorted.sort() \n",
    "    \n",
    "    # using == to check if  \n",
    "    if ref_data_cols_sorted == df_data_cols_sorted:\n",
    "        col_condition_7 = True\n",
    "    else : \n",
    "        col_condition_7 = False\n",
    "    return col_condition_7, scan_num\n",
    "\n",
    "def data_integrity_check(df_summary, df_summary_cols, df_data, df_data_cols, file_initial): \n",
    "    \n",
    "    print(color.BOLD + f\"Reading {file_initial} raw data and perform data integrity scanning...:\\n\" + color.END)\n",
    "      \n",
    "    conditions_list = []\n",
    "    \n",
    "    # SCAN-1\n",
    "    col_condition_1, scan_num = summary_col_check(df_summary, df_summary_cols, file_initial , 'Summary')\n",
    "    print_scan_results(col_condition_1, scan_num, file_initial , sheets = 'Summary')\n",
    "    conditions_list.append(col_condition_1)\n",
    "      \n",
    "    # SCAN-2\n",
    "    # Runs only when col_condition_1 returns True\n",
    "    if col_condition_1 == True:\n",
    "        col_condition_2, scan_num = summary_col_value_check(df_summary, file_initial, 'Summary')  \n",
    "        print_scan_results(col_condition_2, scan_num, file_initial , sheets = 'Summary')\n",
    "        conditions_list.append(col_condition_2)\n",
    "    else:\n",
    "        conditions_list = conditions_list\n",
    "\n",
    "    # SCAN-3\n",
    "    col_condition_3, scan_num = col_header_check(df_summary, file_initial, 'Summary')\n",
    "    print_scan_results(col_condition_3, scan_num, file_initial , sheets = 'Summary')\n",
    "    conditions_list.append(col_condition_3)\n",
    "    \n",
    "    # SCAN-4\n",
    "    col_condition_4, scan_num = data_col_check(df_data, df_data_cols, file_initial, sheets = 'Data')\n",
    "    print_scan_results(col_condition_4, scan_num, file_initial , sheets = 'Data')\n",
    "    conditions_list.append(col_condition_4)\n",
    "    \n",
    "    # SCAN-5\n",
    "    # Runs only when col_condition_4 returns True\n",
    "    if col_condition_4 == True:\n",
    "        col_condition_5, scan_num = data_col_value_check(df_data, file_initial, sheets = 'Data')\n",
    "        print_scan_results(col_condition_5, scan_num, file_initial , sheets = 'Data')\n",
    "        conditions_list.append(col_condition_5)\n",
    "    else:\n",
    "        conditions_list = conditions_list\n",
    "        \n",
    "    # SCAN-6 \n",
    "    col_condition_6, scan_num = col_header_check(df_data, file_initial, 'Data')\n",
    "    scan_num = 6\n",
    "    print_scan_results(col_condition_6, scan_num, file_initial , 'Data')\n",
    "    conditions_list.append(col_condition_6)\n",
    "    \n",
    "    # SCAN-7\n",
    "    if file_initial == 'RC':\n",
    "        col_condition_7, scan_num = data_col_header_check(df_data_cols, ref_data_cols, file_initial, sheets = 'Data')\n",
    "        print_scan_results(col_condition_7, scan_num, file_initial , 'Data')\n",
    "        conditions_list.append(col_condition_7)\n",
    "       \n",
    "    # Final data integrity results after all checks\n",
    "    # PASS -> when all scans return True/PASS\n",
    "    if len(conditions_list) > 1 :\n",
    "        integrity_result = all(conditions_list)\n",
    "        if integrity_result == True:\n",
    "            print(color.BOLD + f'\\n{file_initial} data integrity result:' + color.GREEN + ' PASS' + color.END + '\\n')\n",
    "        else: \n",
    "            print(color.BOLD + f'\\n{file_initial} data integrity result:' + color.RED + ' FAIL' + color.END + '\\n')\n",
    "    elif len(conditions_list) == 1: \n",
    "        print(color.BOLD + f'\\n{file_initial} data integrity result:' + color.RED + ' FAIL' + color.END + '\\n')\n",
    "\n",
    "    return integrity_result, conditions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the data integrity report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe created from RC file\n",
      "\u001b[1mReading RC raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : RC - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[91mFAIL\u001b[0m: 'Summary' sheet does not contain either 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-3 : RC - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : RC - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[91mFAIL\u001b[0m: 'Data' sheet does not contain 'Language' columns\n",
      "\n",
      "SCAN-6 : RC - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\n",
      "SCAN-7 : RC - Data : checking if columns in the 'Data' sheet are identical to the reference columns ...\n",
      "\u001b[91mFAIL\u001b[0m: The columns in the 'Data' sheet are not identical to the reference\n",
      "\u001b[1m\n",
      "RC data integrity result:\u001b[91m FAIL\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_1 file\n",
      "\u001b[1mReading Vocab_1 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_1 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[91mFAIL\u001b[0m: 'Summary' sheet does not contain either 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-3 : Vocab_1 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[91mFAIL\u001b[0m: invalid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_1 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[91mFAIL\u001b[0m: 'Data' sheet does not contain 'Language' columns\n",
      "\n",
      "SCAN-6 : Vocab_1 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_1 data integrity result:\u001b[91m FAIL\u001b[0m\n",
      "\n",
      "Dataframe created from Vocab_2 file\n",
      "\u001b[1mReading Vocab_2 raw data and perform data integrity scanning...:\n",
      "\u001b[0m\n",
      "\n",
      "SCAN-1 : Vocab_2 - Summary : Checking if the sheet contains either 'Language' and 'Market' columns ...\n",
      "\u001b[91mFAIL\u001b[0m: 'Summary' sheet does not contain either 'Language' and 'Market' columns\n",
      "\n",
      "SCAN-3 : Vocab_2 - Summary : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[91mFAIL\u001b[0m: invalid '_workder_id' column name\n",
      "\n",
      "SCAN-4 : Vocab_2 - Data : Checking if sheet contains 'Language' column ...\n",
      "\u001b[91mFAIL\u001b[0m: 'Data' sheet does not contain 'Language' columns\n",
      "\n",
      "SCAN-6 : Vocab_2 - Data : Checking if '_worker_id' column name is correct ...\n",
      "\u001b[92mPASS\u001b[0m: valid '_workder_id' column name\n",
      "\u001b[1m\n",
      "Vocab_2 data integrity result:\u001b[91m FAIL\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    file_initials = ['RC', 'Vocab_1', 'Vocab_2']\n",
    "\n",
    "    int_results = {}\n",
    "    for file_initial in file_initials:\n",
    "        df_summary, df_summary_cols, df_data, df_data_cols, df_ans_key, df_ans_key_cols = create_dataframes(file_initial, rc_filepath, v1_filepath , v2_filepath) \n",
    "        integrity_result, conditions_list = data_integrity_check(df_summary, df_summary_cols, df_data, df_data_cols, file_initial)\n",
    "        int_results[file_initial] = integrity_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALA",
   "language": "python",
   "name": "ala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
